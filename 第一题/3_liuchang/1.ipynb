{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性分类\n",
    "* 不能使用现成机器学习库（比如sklearn, tensorflow, caffe, pytorch等），用numpy实现线性分类\n",
    "* 文件x_train.npy和t_train.npy给出训练数据和真值\n",
    "* 提交对于文件test所给数据的预测结果，以numpy数组的形式存储为文件t_test.npy\n",
    "* 以文本文件的形式提交分类平面的表达式\n",
    "\n",
    "数据演示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f537d60c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "t_train = np.load('t_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], c=t_test_pred)  # maybe the color\n",
    "plt.xlim([-4, 6])\n",
    "plt.ylim([-4, 6])\n",
    "#添加\n",
    "num_training = 75\n",
    "num_validation = 0\n",
    "\n",
    "# n = np.random.permutation(t_train.shape[0])\n",
    "# x_train = x_train[n,:]\n",
    "# t_train = t_train[n]\n",
    "\n",
    "\n",
    "# mask = list(range(num_training, num_training + num_validation))\n",
    "# x_val = x_train[mask]\n",
    "# t_val = t_train[mask]\n",
    "# mask = list(range(num_training))\n",
    "# x_train = x_train[mask]\n",
    "# t_train = t_train[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "# x_val = np.reshape(x_val, (x_val.shape[0], -1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "    \n",
    "# Normalize the data: subtract the mean image\n",
    "mean_image = np.mean(x_train, axis = 0)\n",
    "x_train -= mean_image\n",
    "# x_val -= mean_image\n",
    "x_test -= mean_image\n",
    "    \n",
    "# add bias dimension and transform into columns\n",
    "x_train = np.hstack([x_train, np.ones((x_train.shape[0], 1))])\n",
    "# x_val = np.hstack([x_val, np.ones((x_val.shape[0], 1))])\n",
    "x_test = np.hstack([x_test, np.ones((x_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax import *\n",
    "from linear_classifier import *\n",
    "from linear_classifier import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.096877\n",
      "iteration 100 / 1000: loss 0.351390\n",
      "iteration 200 / 1000: loss 0.477652\n",
      "iteration 300 / 1000: loss 0.438332\n",
      "iteration 400 / 1000: loss 0.398634\n",
      "iteration 500 / 1000: loss 0.418507\n",
      "iteration 600 / 1000: loss 0.467769\n",
      "iteration 700 / 1000: loss 0.422739\n",
      "iteration 800 / 1000: loss 0.434769\n",
      "iteration 900 / 1000: loss 0.423385\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098001\n",
      "iteration 100 / 1000: loss 0.403596\n",
      "iteration 200 / 1000: loss 0.450893\n",
      "iteration 300 / 1000: loss 0.470815\n",
      "iteration 400 / 1000: loss 0.414685\n",
      "iteration 500 / 1000: loss 0.450385\n",
      "iteration 600 / 1000: loss 0.372918\n",
      "iteration 700 / 1000: loss 0.415763\n",
      "iteration 800 / 1000: loss 0.388107\n",
      "iteration 900 / 1000: loss 0.452170\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097064\n",
      "iteration 100 / 1000: loss 0.429092\n",
      "iteration 200 / 1000: loss 0.420191\n",
      "iteration 300 / 1000: loss 0.464782\n",
      "iteration 400 / 1000: loss 0.406240\n",
      "iteration 500 / 1000: loss 0.445417\n",
      "iteration 600 / 1000: loss 0.437534\n",
      "iteration 700 / 1000: loss 0.430688\n",
      "iteration 800 / 1000: loss 0.450617\n",
      "iteration 900 / 1000: loss 0.480791\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100233\n",
      "iteration 100 / 1000: loss 0.400004\n",
      "iteration 200 / 1000: loss 0.451835\n",
      "iteration 300 / 1000: loss 0.458752\n",
      "iteration 400 / 1000: loss 0.457707\n",
      "iteration 500 / 1000: loss 0.454793\n",
      "iteration 600 / 1000: loss 0.393751\n",
      "iteration 700 / 1000: loss 0.424405\n",
      "iteration 800 / 1000: loss 0.443697\n",
      "iteration 900 / 1000: loss 0.457020\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100622\n",
      "iteration 100 / 1000: loss 0.423721\n",
      "iteration 200 / 1000: loss 0.442804\n",
      "iteration 300 / 1000: loss 0.412922\n",
      "iteration 400 / 1000: loss 0.500502\n",
      "iteration 500 / 1000: loss 0.413391\n",
      "iteration 600 / 1000: loss 0.464674\n",
      "iteration 700 / 1000: loss 0.429193\n",
      "iteration 800 / 1000: loss 0.427573\n",
      "iteration 900 / 1000: loss 0.393300\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099087\n",
      "iteration 100 / 1000: loss 0.467668\n",
      "iteration 200 / 1000: loss 0.458107\n",
      "iteration 300 / 1000: loss 0.418676\n",
      "iteration 400 / 1000: loss 0.486481\n",
      "iteration 500 / 1000: loss 0.474937\n",
      "iteration 600 / 1000: loss 0.513151\n",
      "iteration 700 / 1000: loss 0.509935\n",
      "iteration 800 / 1000: loss 0.483854\n",
      "iteration 900 / 1000: loss 0.426483\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.102602\n",
      "iteration 100 / 1000: loss 0.439942\n",
      "iteration 200 / 1000: loss 0.421926\n",
      "iteration 300 / 1000: loss 0.431325\n",
      "iteration 400 / 1000: loss 0.429009\n",
      "iteration 500 / 1000: loss 0.413866\n",
      "iteration 600 / 1000: loss 0.461707\n",
      "iteration 700 / 1000: loss 0.510549\n",
      "iteration 800 / 1000: loss 0.437692\n",
      "iteration 900 / 1000: loss 0.435609\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099138\n",
      "iteration 100 / 1000: loss 0.444564\n",
      "iteration 200 / 1000: loss 0.454329\n",
      "iteration 300 / 1000: loss 0.409815\n",
      "iteration 400 / 1000: loss 0.455510\n",
      "iteration 500 / 1000: loss 0.438809\n",
      "iteration 600 / 1000: loss 0.482510\n",
      "iteration 700 / 1000: loss 0.412191\n",
      "iteration 800 / 1000: loss 0.441928\n",
      "iteration 900 / 1000: loss 0.481431\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099444\n",
      "iteration 100 / 1000: loss 0.463026\n",
      "iteration 200 / 1000: loss 0.481705\n",
      "iteration 300 / 1000: loss 0.457265\n",
      "iteration 400 / 1000: loss 0.474943\n",
      "iteration 500 / 1000: loss 0.480073\n",
      "iteration 600 / 1000: loss 0.514098\n",
      "iteration 700 / 1000: loss 0.464478\n",
      "iteration 800 / 1000: loss 0.497978\n",
      "iteration 900 / 1000: loss 0.473132\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099075\n",
      "iteration 100 / 1000: loss 0.502795\n",
      "iteration 200 / 1000: loss 0.482358\n",
      "iteration 300 / 1000: loss 0.423685\n",
      "iteration 400 / 1000: loss 0.540534\n",
      "iteration 500 / 1000: loss 0.516403\n",
      "iteration 600 / 1000: loss 0.411905\n",
      "iteration 700 / 1000: loss 0.544401\n",
      "iteration 800 / 1000: loss 0.430118\n",
      "iteration 900 / 1000: loss 0.460158\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097135\n",
      "iteration 100 / 1000: loss 0.457433\n",
      "iteration 200 / 1000: loss 0.451471\n",
      "iteration 300 / 1000: loss 0.452329\n",
      "iteration 400 / 1000: loss 0.497481\n",
      "iteration 500 / 1000: loss 0.470968\n",
      "iteration 600 / 1000: loss 0.503737\n",
      "iteration 700 / 1000: loss 0.487437\n",
      "iteration 800 / 1000: loss 0.456790\n",
      "iteration 900 / 1000: loss 0.432915\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096696\n",
      "iteration 100 / 1000: loss 0.501247\n",
      "iteration 200 / 1000: loss 0.466118\n",
      "iteration 300 / 1000: loss 0.477778\n",
      "iteration 400 / 1000: loss 0.457876\n",
      "iteration 500 / 1000: loss 0.528756\n",
      "iteration 600 / 1000: loss 0.489507\n",
      "iteration 700 / 1000: loss 0.453485\n",
      "iteration 800 / 1000: loss 0.481415\n",
      "iteration 900 / 1000: loss 0.457114\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098351\n",
      "iteration 100 / 1000: loss 0.476844\n",
      "iteration 200 / 1000: loss 0.476996\n",
      "iteration 300 / 1000: loss 0.474031\n",
      "iteration 400 / 1000: loss 0.491748\n",
      "iteration 500 / 1000: loss 0.470889\n",
      "iteration 600 / 1000: loss 0.469425\n",
      "iteration 700 / 1000: loss 0.454146\n",
      "iteration 800 / 1000: loss 0.493325\n",
      "iteration 900 / 1000: loss 0.490923\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097224\n",
      "iteration 100 / 1000: loss 0.392321\n",
      "iteration 200 / 1000: loss 0.448934\n",
      "iteration 300 / 1000: loss 0.395477\n",
      "iteration 400 / 1000: loss 0.413578\n",
      "iteration 500 / 1000: loss 0.418240\n",
      "iteration 600 / 1000: loss 0.393846\n",
      "iteration 700 / 1000: loss 0.437154\n",
      "iteration 800 / 1000: loss 0.412219\n",
      "iteration 900 / 1000: loss 0.416725\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096837\n",
      "iteration 100 / 1000: loss 0.423148\n",
      "iteration 200 / 1000: loss 0.428959\n",
      "iteration 300 / 1000: loss 0.443554\n",
      "iteration 400 / 1000: loss 0.430429\n",
      "iteration 500 / 1000: loss 0.454100\n",
      "iteration 600 / 1000: loss 0.416686\n",
      "iteration 700 / 1000: loss 0.410285\n",
      "iteration 800 / 1000: loss 0.471730\n",
      "iteration 900 / 1000: loss 0.448005\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099009\n",
      "iteration 100 / 1000: loss 0.387665\n",
      "iteration 200 / 1000: loss 0.423250\n",
      "iteration 300 / 1000: loss 0.366672\n",
      "iteration 400 / 1000: loss 0.453656\n",
      "iteration 500 / 1000: loss 0.413272\n",
      "iteration 600 / 1000: loss 0.447198\n",
      "iteration 700 / 1000: loss 0.449103\n",
      "iteration 800 / 1000: loss 0.438100\n",
      "iteration 900 / 1000: loss 0.408979\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097467\n",
      "iteration 100 / 1000: loss 0.432835\n",
      "iteration 200 / 1000: loss 0.410148\n",
      "iteration 300 / 1000: loss 0.439226\n",
      "iteration 400 / 1000: loss 0.462080\n",
      "iteration 500 / 1000: loss 0.425154\n",
      "iteration 600 / 1000: loss 0.415540\n",
      "iteration 700 / 1000: loss 0.440261\n",
      "iteration 800 / 1000: loss 0.444228\n",
      "iteration 900 / 1000: loss 0.450653\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098133\n",
      "iteration 100 / 1000: loss 0.404564\n",
      "iteration 200 / 1000: loss 0.415797\n",
      "iteration 300 / 1000: loss 0.436192\n",
      "iteration 400 / 1000: loss 0.449078\n",
      "iteration 500 / 1000: loss 0.470255\n",
      "iteration 600 / 1000: loss 0.426969\n",
      "iteration 700 / 1000: loss 0.431387\n",
      "iteration 800 / 1000: loss 0.458729\n",
      "iteration 900 / 1000: loss 0.485231\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099919\n",
      "iteration 100 / 1000: loss 0.479184\n",
      "iteration 200 / 1000: loss 0.474771\n",
      "iteration 300 / 1000: loss 0.439881\n",
      "iteration 400 / 1000: loss 0.456456\n",
      "iteration 500 / 1000: loss 0.521412\n",
      "iteration 600 / 1000: loss 0.450313\n",
      "iteration 700 / 1000: loss 0.473469\n",
      "iteration 800 / 1000: loss 0.481882\n",
      "iteration 900 / 1000: loss 0.442171\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099548\n",
      "iteration 100 / 1000: loss 0.458911\n",
      "iteration 200 / 1000: loss 0.431141\n",
      "iteration 300 / 1000: loss 0.426864\n",
      "iteration 400 / 1000: loss 0.454490\n",
      "iteration 500 / 1000: loss 0.510599\n",
      "iteration 600 / 1000: loss 0.479600\n",
      "iteration 700 / 1000: loss 0.442522\n",
      "iteration 800 / 1000: loss 0.445471\n",
      "iteration 900 / 1000: loss 0.395865\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097778\n",
      "iteration 100 / 1000: loss 0.429808\n",
      "iteration 200 / 1000: loss 0.466126\n",
      "iteration 300 / 1000: loss 0.425446\n",
      "iteration 400 / 1000: loss 0.511622\n",
      "iteration 500 / 1000: loss 0.541046\n",
      "iteration 600 / 1000: loss 0.438020\n",
      "iteration 700 / 1000: loss 0.451428\n",
      "iteration 800 / 1000: loss 0.455333\n",
      "iteration 900 / 1000: loss 0.469421\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100164\n",
      "iteration 100 / 1000: loss 0.479915\n",
      "iteration 200 / 1000: loss 0.480028\n",
      "iteration 300 / 1000: loss 0.446071\n",
      "iteration 400 / 1000: loss 0.457876\n",
      "iteration 500 / 1000: loss 0.481681\n",
      "iteration 600 / 1000: loss 0.458282\n",
      "iteration 700 / 1000: loss 0.455192\n",
      "iteration 800 / 1000: loss 0.466087\n",
      "iteration 900 / 1000: loss 0.460850\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095903\n",
      "iteration 100 / 1000: loss 0.452974\n",
      "iteration 200 / 1000: loss 0.466816\n",
      "iteration 300 / 1000: loss 0.500453\n",
      "iteration 400 / 1000: loss 0.503902\n",
      "iteration 500 / 1000: loss 0.482782\n",
      "iteration 600 / 1000: loss 0.442131\n",
      "iteration 700 / 1000: loss 0.508574\n",
      "iteration 800 / 1000: loss 0.487844\n",
      "iteration 900 / 1000: loss 0.444978\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099557\n",
      "iteration 100 / 1000: loss 0.510400\n",
      "iteration 200 / 1000: loss 0.462809\n",
      "iteration 300 / 1000: loss 0.461359\n",
      "iteration 400 / 1000: loss 0.466181\n",
      "iteration 500 / 1000: loss 0.471731\n",
      "iteration 600 / 1000: loss 0.481147\n",
      "iteration 700 / 1000: loss 0.460645\n",
      "iteration 800 / 1000: loss 0.482739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1000: loss 0.440951\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098187\n",
      "iteration 100 / 1000: loss 0.514452\n",
      "iteration 200 / 1000: loss 0.470507\n",
      "iteration 300 / 1000: loss 0.422321\n",
      "iteration 400 / 1000: loss 0.437842\n",
      "iteration 500 / 1000: loss 0.501007\n",
      "iteration 600 / 1000: loss 0.412349\n",
      "iteration 700 / 1000: loss 0.458260\n",
      "iteration 800 / 1000: loss 0.470054\n",
      "iteration 900 / 1000: loss 0.472368\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098397\n",
      "iteration 100 / 1000: loss 0.469853\n",
      "iteration 200 / 1000: loss 0.469685\n",
      "iteration 300 / 1000: loss 0.445381\n",
      "iteration 400 / 1000: loss 0.447094\n",
      "iteration 500 / 1000: loss 0.451110\n",
      "iteration 600 / 1000: loss 0.486904\n",
      "iteration 700 / 1000: loss 0.491914\n",
      "iteration 800 / 1000: loss 0.446798\n",
      "iteration 900 / 1000: loss 0.466899\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098664\n",
      "iteration 100 / 1000: loss 0.422426\n",
      "iteration 200 / 1000: loss 0.444708\n",
      "iteration 300 / 1000: loss 0.407668\n",
      "iteration 400 / 1000: loss 0.416928\n",
      "iteration 500 / 1000: loss 0.443226\n",
      "iteration 600 / 1000: loss 0.422689\n",
      "iteration 700 / 1000: loss 0.451657\n",
      "iteration 800 / 1000: loss 0.383869\n",
      "iteration 900 / 1000: loss 0.446219\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098870\n",
      "iteration 100 / 1000: loss 0.441055\n",
      "iteration 200 / 1000: loss 0.455479\n",
      "iteration 300 / 1000: loss 0.411795\n",
      "iteration 400 / 1000: loss 0.405887\n",
      "iteration 500 / 1000: loss 0.417971\n",
      "iteration 600 / 1000: loss 0.381976\n",
      "iteration 700 / 1000: loss 0.416070\n",
      "iteration 800 / 1000: loss 0.412729\n",
      "iteration 900 / 1000: loss 0.412581\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096848\n",
      "iteration 100 / 1000: loss 0.442786\n",
      "iteration 200 / 1000: loss 0.429779\n",
      "iteration 300 / 1000: loss 0.430469\n",
      "iteration 400 / 1000: loss 0.413725\n",
      "iteration 500 / 1000: loss 0.431498\n",
      "iteration 600 / 1000: loss 0.426959\n",
      "iteration 700 / 1000: loss 0.387591\n",
      "iteration 800 / 1000: loss 0.476681\n",
      "iteration 900 / 1000: loss 0.433573\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099517\n",
      "iteration 100 / 1000: loss 0.442466\n",
      "iteration 200 / 1000: loss 0.465755\n",
      "iteration 300 / 1000: loss 0.467054\n",
      "iteration 400 / 1000: loss 0.440232\n",
      "iteration 500 / 1000: loss 0.450023\n",
      "iteration 600 / 1000: loss 0.484687\n",
      "iteration 700 / 1000: loss 0.407671\n",
      "iteration 800 / 1000: loss 0.420937\n",
      "iteration 900 / 1000: loss 0.411969\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097726\n",
      "iteration 100 / 1000: loss 0.455341\n",
      "iteration 200 / 1000: loss 0.476190\n",
      "iteration 300 / 1000: loss 0.450423\n",
      "iteration 400 / 1000: loss 0.455902\n",
      "iteration 500 / 1000: loss 0.449000\n",
      "iteration 600 / 1000: loss 0.469178\n",
      "iteration 700 / 1000: loss 0.456288\n",
      "iteration 800 / 1000: loss 0.417270\n",
      "iteration 900 / 1000: loss 0.420025\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096120\n",
      "iteration 100 / 1000: loss 0.435798\n",
      "iteration 200 / 1000: loss 0.473213\n",
      "iteration 300 / 1000: loss 0.435649\n",
      "iteration 400 / 1000: loss 0.409556\n",
      "iteration 500 / 1000: loss 0.441896\n",
      "iteration 600 / 1000: loss 0.468131\n",
      "iteration 700 / 1000: loss 0.438425\n",
      "iteration 800 / 1000: loss 0.459003\n",
      "iteration 900 / 1000: loss 0.405996\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.101692\n",
      "iteration 100 / 1000: loss 0.475185\n",
      "iteration 200 / 1000: loss 0.418921\n",
      "iteration 300 / 1000: loss 0.456717\n",
      "iteration 400 / 1000: loss 0.470825\n",
      "iteration 500 / 1000: loss 0.479478\n",
      "iteration 600 / 1000: loss 0.468675\n",
      "iteration 700 / 1000: loss 0.459477\n",
      "iteration 800 / 1000: loss 0.490158\n",
      "iteration 900 / 1000: loss 0.448563\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098225\n",
      "iteration 100 / 1000: loss 0.438098\n",
      "iteration 200 / 1000: loss 0.427474\n",
      "iteration 300 / 1000: loss 0.541037\n",
      "iteration 400 / 1000: loss 0.473532\n",
      "iteration 500 / 1000: loss 0.472615\n",
      "iteration 600 / 1000: loss 0.408139\n",
      "iteration 700 / 1000: loss 0.450340\n",
      "iteration 800 / 1000: loss 0.472901\n",
      "iteration 900 / 1000: loss 0.506427\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097771\n",
      "iteration 100 / 1000: loss 0.487950\n",
      "iteration 200 / 1000: loss 0.478384\n",
      "iteration 300 / 1000: loss 0.507959\n",
      "iteration 400 / 1000: loss 0.493512\n",
      "iteration 500 / 1000: loss 0.510954\n",
      "iteration 600 / 1000: loss 0.512247\n",
      "iteration 700 / 1000: loss 0.471094\n",
      "iteration 800 / 1000: loss 0.436173\n",
      "iteration 900 / 1000: loss 0.482872\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100878\n",
      "iteration 100 / 1000: loss 0.463180\n",
      "iteration 200 / 1000: loss 0.461403\n",
      "iteration 300 / 1000: loss 0.464146\n",
      "iteration 400 / 1000: loss 0.468940\n",
      "iteration 500 / 1000: loss 0.475544\n",
      "iteration 600 / 1000: loss 0.467297\n",
      "iteration 700 / 1000: loss 0.538931\n",
      "iteration 800 / 1000: loss 0.462276\n",
      "iteration 900 / 1000: loss 0.451008\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096857\n",
      "iteration 100 / 1000: loss 0.440525\n",
      "iteration 200 / 1000: loss 0.430862\n",
      "iteration 300 / 1000: loss 0.475748\n",
      "iteration 400 / 1000: loss 0.461712\n",
      "iteration 500 / 1000: loss 0.476037\n",
      "iteration 600 / 1000: loss 0.437223\n",
      "iteration 700 / 1000: loss 0.493085\n",
      "iteration 800 / 1000: loss 0.481405\n",
      "iteration 900 / 1000: loss 0.435631\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098665\n",
      "iteration 100 / 1000: loss 0.442320\n",
      "iteration 200 / 1000: loss 0.455497\n",
      "iteration 300 / 1000: loss 0.442794\n",
      "iteration 400 / 1000: loss 0.449457\n",
      "iteration 500 / 1000: loss 0.483452\n",
      "iteration 600 / 1000: loss 0.486246\n",
      "iteration 700 / 1000: loss 0.511176\n",
      "iteration 800 / 1000: loss 0.450131\n",
      "iteration 900 / 1000: loss 0.481787\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098555\n",
      "iteration 100 / 1000: loss 0.555830\n",
      "iteration 200 / 1000: loss 0.436475\n",
      "iteration 300 / 1000: loss 0.500356\n",
      "iteration 400 / 1000: loss 0.512748\n",
      "iteration 500 / 1000: loss 0.471222\n",
      "iteration 600 / 1000: loss 0.457896\n",
      "iteration 700 / 1000: loss 0.476959\n",
      "iteration 800 / 1000: loss 0.466984\n",
      "iteration 900 / 1000: loss 0.541133\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100064\n",
      "iteration 100 / 1000: loss 0.410846\n",
      "iteration 200 / 1000: loss 0.427638\n",
      "iteration 300 / 1000: loss 0.392113\n",
      "iteration 400 / 1000: loss 0.418615\n",
      "iteration 500 / 1000: loss 0.404763\n",
      "iteration 600 / 1000: loss 0.413568\n",
      "iteration 700 / 1000: loss 0.445817\n",
      "iteration 800 / 1000: loss 0.477640\n",
      "iteration 900 / 1000: loss 0.432213\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096318\n",
      "iteration 100 / 1000: loss 0.397838\n",
      "iteration 200 / 1000: loss 0.410212\n",
      "iteration 300 / 1000: loss 0.447433\n",
      "iteration 400 / 1000: loss 0.481222\n",
      "iteration 500 / 1000: loss 0.464262\n",
      "iteration 600 / 1000: loss 0.422654\n",
      "iteration 700 / 1000: loss 0.431416\n",
      "iteration 800 / 1000: loss 0.455683\n",
      "iteration 900 / 1000: loss 0.421539\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099075\n",
      "iteration 100 / 1000: loss 0.411082\n",
      "iteration 200 / 1000: loss 0.450839\n",
      "iteration 300 / 1000: loss 0.461551\n",
      "iteration 400 / 1000: loss 0.435448\n",
      "iteration 500 / 1000: loss 0.482659\n",
      "iteration 600 / 1000: loss 0.446079\n",
      "iteration 700 / 1000: loss 0.476450\n",
      "iteration 800 / 1000: loss 0.448126\n",
      "iteration 900 / 1000: loss 0.437288\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098629\n",
      "iteration 100 / 1000: loss 0.466320\n",
      "iteration 200 / 1000: loss 0.503753\n",
      "iteration 300 / 1000: loss 0.423713\n",
      "iteration 400 / 1000: loss 0.469487\n",
      "iteration 500 / 1000: loss 0.466041\n",
      "iteration 600 / 1000: loss 0.448132\n",
      "iteration 700 / 1000: loss 0.413218\n",
      "iteration 800 / 1000: loss 0.436426\n",
      "iteration 900 / 1000: loss 0.475326\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.101115\n",
      "iteration 100 / 1000: loss 0.477204\n",
      "iteration 200 / 1000: loss 0.428529\n",
      "iteration 300 / 1000: loss 0.449089\n",
      "iteration 400 / 1000: loss 0.407960\n",
      "iteration 500 / 1000: loss 0.434150\n",
      "iteration 600 / 1000: loss 0.392176\n",
      "iteration 700 / 1000: loss 0.411460\n",
      "iteration 800 / 1000: loss 0.437978\n",
      "iteration 900 / 1000: loss 0.456621\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097998\n",
      "iteration 100 / 1000: loss 0.410045\n",
      "iteration 200 / 1000: loss 0.432825\n",
      "iteration 300 / 1000: loss 0.438195\n",
      "iteration 400 / 1000: loss 0.399000\n",
      "iteration 500 / 1000: loss 0.469744\n",
      "iteration 600 / 1000: loss 0.403018\n",
      "iteration 700 / 1000: loss 0.479829\n",
      "iteration 800 / 1000: loss 0.467527\n",
      "iteration 900 / 1000: loss 0.423351\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100741\n",
      "iteration 100 / 1000: loss 0.493609\n",
      "iteration 200 / 1000: loss 0.391346\n",
      "iteration 300 / 1000: loss 0.472835\n",
      "iteration 400 / 1000: loss 0.475054\n",
      "iteration 500 / 1000: loss 0.435846\n",
      "iteration 600 / 1000: loss 0.444531\n",
      "iteration 700 / 1000: loss 0.443539\n",
      "iteration 800 / 1000: loss 0.477732\n",
      "iteration 900 / 1000: loss 0.410237\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098812\n",
      "iteration 100 / 1000: loss 0.478179\n",
      "iteration 200 / 1000: loss 0.443387\n",
      "iteration 300 / 1000: loss 0.482650\n",
      "iteration 400 / 1000: loss 0.451106\n",
      "iteration 500 / 1000: loss 0.503305\n",
      "iteration 600 / 1000: loss 0.456297\n",
      "iteration 700 / 1000: loss 0.484042\n",
      "iteration 800 / 1000: loss 0.445983\n",
      "iteration 900 / 1000: loss 0.488053\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096754\n",
      "iteration 100 / 1000: loss 0.452256\n",
      "iteration 200 / 1000: loss 0.497270\n",
      "iteration 300 / 1000: loss 0.455869\n",
      "iteration 400 / 1000: loss 0.435240\n",
      "iteration 500 / 1000: loss 0.426910\n",
      "iteration 600 / 1000: loss 0.502695\n",
      "iteration 700 / 1000: loss 0.446591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.505146\n",
      "iteration 900 / 1000: loss 0.511183\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098219\n",
      "iteration 100 / 1000: loss 0.443330\n",
      "iteration 200 / 1000: loss 0.426507\n",
      "iteration 300 / 1000: loss 0.500727\n",
      "iteration 400 / 1000: loss 0.456091\n",
      "iteration 500 / 1000: loss 0.462395\n",
      "iteration 600 / 1000: loss 0.469629\n",
      "iteration 700 / 1000: loss 0.488987\n",
      "iteration 800 / 1000: loss 0.467603\n",
      "iteration 900 / 1000: loss 0.441051\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099318\n",
      "iteration 100 / 1000: loss 0.490983\n",
      "iteration 200 / 1000: loss 0.459586\n",
      "iteration 300 / 1000: loss 0.460900\n",
      "iteration 400 / 1000: loss 0.520591\n",
      "iteration 500 / 1000: loss 0.479618\n",
      "iteration 600 / 1000: loss 0.461113\n",
      "iteration 700 / 1000: loss 0.479899\n",
      "iteration 800 / 1000: loss 0.515603\n",
      "iteration 900 / 1000: loss 0.449677\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099795\n",
      "iteration 100 / 1000: loss 0.507793\n",
      "iteration 200 / 1000: loss 0.483442\n",
      "iteration 300 / 1000: loss 0.477699\n",
      "iteration 400 / 1000: loss 0.478971\n",
      "iteration 500 / 1000: loss 0.423715\n",
      "iteration 600 / 1000: loss 0.464588\n",
      "iteration 700 / 1000: loss 0.497297\n",
      "iteration 800 / 1000: loss 0.452464\n",
      "iteration 900 / 1000: loss 0.444102\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098733\n",
      "iteration 100 / 1000: loss 0.482407\n",
      "iteration 200 / 1000: loss 0.473660\n",
      "iteration 300 / 1000: loss 0.438691\n",
      "iteration 400 / 1000: loss 0.487021\n",
      "iteration 500 / 1000: loss 0.554794\n",
      "iteration 600 / 1000: loss 0.449794\n",
      "iteration 700 / 1000: loss 0.443058\n",
      "iteration 800 / 1000: loss 0.431334\n",
      "iteration 900 / 1000: loss 0.465289\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097561\n",
      "iteration 100 / 1000: loss 0.381078\n",
      "iteration 200 / 1000: loss 0.396420\n",
      "iteration 300 / 1000: loss 0.422627\n",
      "iteration 400 / 1000: loss 0.411273\n",
      "iteration 500 / 1000: loss 0.470450\n",
      "iteration 600 / 1000: loss 0.405347\n",
      "iteration 700 / 1000: loss 0.381058\n",
      "iteration 800 / 1000: loss 0.381485\n",
      "iteration 900 / 1000: loss 0.433444\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097277\n",
      "iteration 100 / 1000: loss 0.437060\n",
      "iteration 200 / 1000: loss 0.485860\n",
      "iteration 300 / 1000: loss 0.405891\n",
      "iteration 400 / 1000: loss 0.419509\n",
      "iteration 500 / 1000: loss 0.466653\n",
      "iteration 600 / 1000: loss 0.491407\n",
      "iteration 700 / 1000: loss 0.478539\n",
      "iteration 800 / 1000: loss 0.429512\n",
      "iteration 900 / 1000: loss 0.413457\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096628\n",
      "iteration 100 / 1000: loss 0.411064\n",
      "iteration 200 / 1000: loss 0.443587\n",
      "iteration 300 / 1000: loss 0.449040\n",
      "iteration 400 / 1000: loss 0.460792\n",
      "iteration 500 / 1000: loss 0.408399\n",
      "iteration 600 / 1000: loss 0.441011\n",
      "iteration 700 / 1000: loss 0.420120\n",
      "iteration 800 / 1000: loss 0.429101\n",
      "iteration 900 / 1000: loss 0.431186\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099784\n",
      "iteration 100 / 1000: loss 0.446366\n",
      "iteration 200 / 1000: loss 0.433777\n",
      "iteration 300 / 1000: loss 0.464357\n",
      "iteration 400 / 1000: loss 0.435694\n",
      "iteration 500 / 1000: loss 0.438508\n",
      "iteration 600 / 1000: loss 0.430000\n",
      "iteration 700 / 1000: loss 0.447321\n",
      "iteration 800 / 1000: loss 0.413172\n",
      "iteration 900 / 1000: loss 0.421237\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099962\n",
      "iteration 100 / 1000: loss 0.453788\n",
      "iteration 200 / 1000: loss 0.468833\n",
      "iteration 300 / 1000: loss 0.541214\n",
      "iteration 400 / 1000: loss 0.476866\n",
      "iteration 500 / 1000: loss 0.406747\n",
      "iteration 600 / 1000: loss 0.429946\n",
      "iteration 700 / 1000: loss 0.440696\n",
      "iteration 800 / 1000: loss 0.429025\n",
      "iteration 900 / 1000: loss 0.466999\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098760\n",
      "iteration 100 / 1000: loss 0.498082\n",
      "iteration 200 / 1000: loss 0.445639\n",
      "iteration 300 / 1000: loss 0.443144\n",
      "iteration 400 / 1000: loss 0.403655\n",
      "iteration 500 / 1000: loss 0.454105\n",
      "iteration 600 / 1000: loss 0.485072\n",
      "iteration 700 / 1000: loss 0.416170\n",
      "iteration 800 / 1000: loss 0.479333\n",
      "iteration 900 / 1000: loss 0.436186\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098911\n",
      "iteration 100 / 1000: loss 0.443852\n",
      "iteration 200 / 1000: loss 0.489226\n",
      "iteration 300 / 1000: loss 0.450735\n",
      "iteration 400 / 1000: loss 0.443713\n",
      "iteration 500 / 1000: loss 0.435718\n",
      "iteration 600 / 1000: loss 0.442233\n",
      "iteration 700 / 1000: loss 0.457750\n",
      "iteration 800 / 1000: loss 0.471926\n",
      "iteration 900 / 1000: loss 0.422921\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099594\n",
      "iteration 100 / 1000: loss 0.442768\n",
      "iteration 200 / 1000: loss 0.447096\n",
      "iteration 300 / 1000: loss 0.432540\n",
      "iteration 400 / 1000: loss 0.470462\n",
      "iteration 500 / 1000: loss 0.477702\n",
      "iteration 600 / 1000: loss 0.453430\n",
      "iteration 700 / 1000: loss 0.502750\n",
      "iteration 800 / 1000: loss 0.443596\n",
      "iteration 900 / 1000: loss 0.491518\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099133\n",
      "iteration 100 / 1000: loss 0.457031\n",
      "iteration 200 / 1000: loss 0.467283\n",
      "iteration 300 / 1000: loss 0.479553\n",
      "iteration 400 / 1000: loss 0.457790\n",
      "iteration 500 / 1000: loss 0.407003\n",
      "iteration 600 / 1000: loss 0.498016\n",
      "iteration 700 / 1000: loss 0.492119\n",
      "iteration 800 / 1000: loss 0.470197\n",
      "iteration 900 / 1000: loss 0.458679\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098292\n",
      "iteration 100 / 1000: loss 0.484737\n",
      "iteration 200 / 1000: loss 0.418280\n",
      "iteration 300 / 1000: loss 0.501775\n",
      "iteration 400 / 1000: loss 0.476314\n",
      "iteration 500 / 1000: loss 0.440271\n",
      "iteration 600 / 1000: loss 0.448495\n",
      "iteration 700 / 1000: loss 0.505416\n",
      "iteration 800 / 1000: loss 0.504221\n",
      "iteration 900 / 1000: loss 0.456361\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098480\n",
      "iteration 100 / 1000: loss 0.502261\n",
      "iteration 200 / 1000: loss 0.461064\n",
      "iteration 300 / 1000: loss 0.490091\n",
      "iteration 400 / 1000: loss 0.462275\n",
      "iteration 500 / 1000: loss 0.469717\n",
      "iteration 600 / 1000: loss 0.524678\n",
      "iteration 700 / 1000: loss 0.474169\n",
      "iteration 800 / 1000: loss 0.503919\n",
      "iteration 900 / 1000: loss 0.490072\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099933\n",
      "iteration 100 / 1000: loss 0.460355\n",
      "iteration 200 / 1000: loss 0.444808\n",
      "iteration 300 / 1000: loss 0.504986\n",
      "iteration 400 / 1000: loss 0.506432\n",
      "iteration 500 / 1000: loss 0.477429\n",
      "iteration 600 / 1000: loss 0.455725\n",
      "iteration 700 / 1000: loss 0.475105\n",
      "iteration 800 / 1000: loss 0.410753\n",
      "iteration 900 / 1000: loss 0.507650\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099939\n",
      "iteration 100 / 1000: loss 0.486238\n",
      "iteration 200 / 1000: loss 0.479549\n",
      "iteration 300 / 1000: loss 0.513167\n",
      "iteration 400 / 1000: loss 0.480069\n",
      "iteration 500 / 1000: loss 0.460432\n",
      "iteration 600 / 1000: loss 0.491381\n",
      "iteration 700 / 1000: loss 0.485499\n",
      "iteration 800 / 1000: loss 0.424986\n",
      "iteration 900 / 1000: loss 0.489074\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098199\n",
      "iteration 100 / 1000: loss 0.420084\n",
      "iteration 200 / 1000: loss 0.351390\n",
      "iteration 300 / 1000: loss 0.396556\n",
      "iteration 400 / 1000: loss 0.446441\n",
      "iteration 500 / 1000: loss 0.430263\n",
      "iteration 600 / 1000: loss 0.357434\n",
      "iteration 700 / 1000: loss 0.426210\n",
      "iteration 800 / 1000: loss 0.393160\n",
      "iteration 900 / 1000: loss 0.433371\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100289\n",
      "iteration 100 / 1000: loss 0.419213\n",
      "iteration 200 / 1000: loss 0.419281\n",
      "iteration 300 / 1000: loss 0.433890\n",
      "iteration 400 / 1000: loss 0.413227\n",
      "iteration 500 / 1000: loss 0.417072\n",
      "iteration 600 / 1000: loss 0.395108\n",
      "iteration 700 / 1000: loss 0.466386\n",
      "iteration 800 / 1000: loss 0.397150\n",
      "iteration 900 / 1000: loss 0.491967\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098931\n",
      "iteration 100 / 1000: loss 0.399010\n",
      "iteration 200 / 1000: loss 0.435798\n",
      "iteration 300 / 1000: loss 0.466171\n",
      "iteration 400 / 1000: loss 0.447072\n",
      "iteration 500 / 1000: loss 0.430641\n",
      "iteration 600 / 1000: loss 0.428457\n",
      "iteration 700 / 1000: loss 0.432617\n",
      "iteration 800 / 1000: loss 0.438419\n",
      "iteration 900 / 1000: loss 0.446660\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097828\n",
      "iteration 100 / 1000: loss 0.429138\n",
      "iteration 200 / 1000: loss 0.457104\n",
      "iteration 300 / 1000: loss 0.432597\n",
      "iteration 400 / 1000: loss 0.438578\n",
      "iteration 500 / 1000: loss 0.491674\n",
      "iteration 600 / 1000: loss 0.475420\n",
      "iteration 700 / 1000: loss 0.433395\n",
      "iteration 800 / 1000: loss 0.414693\n",
      "iteration 900 / 1000: loss 0.420934\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.102319\n",
      "iteration 100 / 1000: loss 0.477674\n",
      "iteration 200 / 1000: loss 0.447695\n",
      "iteration 300 / 1000: loss 0.458610\n",
      "iteration 400 / 1000: loss 0.430101\n",
      "iteration 500 / 1000: loss 0.449029\n",
      "iteration 600 / 1000: loss 0.477124\n",
      "iteration 700 / 1000: loss 0.430040\n",
      "iteration 800 / 1000: loss 0.465359\n",
      "iteration 900 / 1000: loss 0.448699\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096299\n",
      "iteration 100 / 1000: loss 0.446234\n",
      "iteration 200 / 1000: loss 0.404080\n",
      "iteration 300 / 1000: loss 0.433779\n",
      "iteration 400 / 1000: loss 0.429026\n",
      "iteration 500 / 1000: loss 0.479136\n",
      "iteration 600 / 1000: loss 0.442391\n",
      "iteration 700 / 1000: loss 0.455963\n",
      "iteration 800 / 1000: loss 0.493576\n",
      "iteration 900 / 1000: loss 0.494532\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100089\n",
      "iteration 100 / 1000: loss 0.521159\n",
      "iteration 200 / 1000: loss 0.460885\n",
      "iteration 300 / 1000: loss 0.466417\n",
      "iteration 400 / 1000: loss 0.463630\n",
      "iteration 500 / 1000: loss 0.495447\n",
      "iteration 600 / 1000: loss 0.431864\n",
      "iteration 700 / 1000: loss 0.440830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.430998\n",
      "iteration 900 / 1000: loss 0.435077\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098762\n",
      "iteration 100 / 1000: loss 0.456140\n",
      "iteration 200 / 1000: loss 0.467980\n",
      "iteration 300 / 1000: loss 0.484427\n",
      "iteration 400 / 1000: loss 0.464595\n",
      "iteration 500 / 1000: loss 0.482752\n",
      "iteration 600 / 1000: loss 0.425586\n",
      "iteration 700 / 1000: loss 0.458577\n",
      "iteration 800 / 1000: loss 0.428632\n",
      "iteration 900 / 1000: loss 0.489302\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099167\n",
      "iteration 100 / 1000: loss 0.423831\n",
      "iteration 200 / 1000: loss 0.464177\n",
      "iteration 300 / 1000: loss 0.488949\n",
      "iteration 400 / 1000: loss 0.485167\n",
      "iteration 500 / 1000: loss 0.437606\n",
      "iteration 600 / 1000: loss 0.494686\n",
      "iteration 700 / 1000: loss 0.465642\n",
      "iteration 800 / 1000: loss 0.466570\n",
      "iteration 900 / 1000: loss 0.460634\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099254\n",
      "iteration 100 / 1000: loss 0.442936\n",
      "iteration 200 / 1000: loss 0.490299\n",
      "iteration 300 / 1000: loss 0.491502\n",
      "iteration 400 / 1000: loss 0.486481\n",
      "iteration 500 / 1000: loss 0.459559\n",
      "iteration 600 / 1000: loss 0.418268\n",
      "iteration 700 / 1000: loss 0.483833\n",
      "iteration 800 / 1000: loss 0.446948\n",
      "iteration 900 / 1000: loss 0.459479\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098074\n",
      "iteration 100 / 1000: loss 0.459894\n",
      "iteration 200 / 1000: loss 0.488622\n",
      "iteration 300 / 1000: loss 0.522944\n",
      "iteration 400 / 1000: loss 0.468281\n",
      "iteration 500 / 1000: loss 0.505038\n",
      "iteration 600 / 1000: loss 0.430149\n",
      "iteration 700 / 1000: loss 0.509503\n",
      "iteration 800 / 1000: loss 0.509481\n",
      "iteration 900 / 1000: loss 0.419277\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096230\n",
      "iteration 100 / 1000: loss 0.485823\n",
      "iteration 200 / 1000: loss 0.491693\n",
      "iteration 300 / 1000: loss 0.451273\n",
      "iteration 400 / 1000: loss 0.513673\n",
      "iteration 500 / 1000: loss 0.481631\n",
      "iteration 600 / 1000: loss 0.458230\n",
      "iteration 700 / 1000: loss 0.478204\n",
      "iteration 800 / 1000: loss 0.474708\n",
      "iteration 900 / 1000: loss 0.455792\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096792\n",
      "iteration 100 / 1000: loss 0.459943\n",
      "iteration 200 / 1000: loss 0.523311\n",
      "iteration 300 / 1000: loss 0.514264\n",
      "iteration 400 / 1000: loss 0.479210\n",
      "iteration 500 / 1000: loss 0.463167\n",
      "iteration 600 / 1000: loss 0.537807\n",
      "iteration 700 / 1000: loss 0.496672\n",
      "iteration 800 / 1000: loss 0.472693\n",
      "iteration 900 / 1000: loss 0.508302\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099399\n",
      "iteration 100 / 1000: loss 0.475833\n",
      "iteration 200 / 1000: loss 0.459127\n",
      "iteration 300 / 1000: loss 0.436217\n",
      "iteration 400 / 1000: loss 0.401881\n",
      "iteration 500 / 1000: loss 0.427777\n",
      "iteration 600 / 1000: loss 0.425894\n",
      "iteration 700 / 1000: loss 0.462379\n",
      "iteration 800 / 1000: loss 0.434185\n",
      "iteration 900 / 1000: loss 0.419334\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098037\n",
      "iteration 100 / 1000: loss 0.441895\n",
      "iteration 200 / 1000: loss 0.436119\n",
      "iteration 300 / 1000: loss 0.452777\n",
      "iteration 400 / 1000: loss 0.437271\n",
      "iteration 500 / 1000: loss 0.423644\n",
      "iteration 600 / 1000: loss 0.400929\n",
      "iteration 700 / 1000: loss 0.421878\n",
      "iteration 800 / 1000: loss 0.416171\n",
      "iteration 900 / 1000: loss 0.420489\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097649\n",
      "iteration 100 / 1000: loss 0.458454\n",
      "iteration 200 / 1000: loss 0.432196\n",
      "iteration 300 / 1000: loss 0.414509\n",
      "iteration 400 / 1000: loss 0.420003\n",
      "iteration 500 / 1000: loss 0.492834\n",
      "iteration 600 / 1000: loss 0.428670\n",
      "iteration 700 / 1000: loss 0.439204\n",
      "iteration 800 / 1000: loss 0.415414\n",
      "iteration 900 / 1000: loss 0.478057\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097955\n",
      "iteration 100 / 1000: loss 0.442245\n",
      "iteration 200 / 1000: loss 0.433627\n",
      "iteration 300 / 1000: loss 0.462018\n",
      "iteration 400 / 1000: loss 0.425445\n",
      "iteration 500 / 1000: loss 0.412779\n",
      "iteration 600 / 1000: loss 0.470645\n",
      "iteration 700 / 1000: loss 0.433740\n",
      "iteration 800 / 1000: loss 0.445104\n",
      "iteration 900 / 1000: loss 0.415802\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099343\n",
      "iteration 100 / 1000: loss 0.479670\n",
      "iteration 200 / 1000: loss 0.436291\n",
      "iteration 300 / 1000: loss 0.434619\n",
      "iteration 400 / 1000: loss 0.462286\n",
      "iteration 500 / 1000: loss 0.438440\n",
      "iteration 600 / 1000: loss 0.462402\n",
      "iteration 700 / 1000: loss 0.424571\n",
      "iteration 800 / 1000: loss 0.457748\n",
      "iteration 900 / 1000: loss 0.491865\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098053\n",
      "iteration 100 / 1000: loss 0.476866\n",
      "iteration 200 / 1000: loss 0.438834\n",
      "iteration 300 / 1000: loss 0.482057\n",
      "iteration 400 / 1000: loss 0.439525\n",
      "iteration 500 / 1000: loss 0.477357\n",
      "iteration 600 / 1000: loss 0.499773\n",
      "iteration 700 / 1000: loss 0.473386\n",
      "iteration 800 / 1000: loss 0.466186\n",
      "iteration 900 / 1000: loss 0.466146\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097510\n",
      "iteration 100 / 1000: loss 0.450069\n",
      "iteration 200 / 1000: loss 0.469248\n",
      "iteration 300 / 1000: loss 0.508859\n",
      "iteration 400 / 1000: loss 0.427392\n",
      "iteration 500 / 1000: loss 0.404921\n",
      "iteration 600 / 1000: loss 0.444428\n",
      "iteration 700 / 1000: loss 0.453435\n",
      "iteration 800 / 1000: loss 0.497338\n",
      "iteration 900 / 1000: loss 0.488811\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097219\n",
      "iteration 100 / 1000: loss 0.443055\n",
      "iteration 200 / 1000: loss 0.476538\n",
      "iteration 300 / 1000: loss 0.481989\n",
      "iteration 400 / 1000: loss 0.502118\n",
      "iteration 500 / 1000: loss 0.451325\n",
      "iteration 600 / 1000: loss 0.424334\n",
      "iteration 700 / 1000: loss 0.448694\n",
      "iteration 800 / 1000: loss 0.437307\n",
      "iteration 900 / 1000: loss 0.448110\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097611\n",
      "iteration 100 / 1000: loss 0.460873\n",
      "iteration 200 / 1000: loss 0.487240\n",
      "iteration 300 / 1000: loss 0.470285\n",
      "iteration 400 / 1000: loss 0.440909\n",
      "iteration 500 / 1000: loss 0.447399\n",
      "iteration 600 / 1000: loss 0.474772\n",
      "iteration 700 / 1000: loss 0.455950\n",
      "iteration 800 / 1000: loss 0.493752\n",
      "iteration 900 / 1000: loss 0.480466\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098663\n",
      "iteration 100 / 1000: loss 0.431052\n",
      "iteration 200 / 1000: loss 0.436476\n",
      "iteration 300 / 1000: loss 0.484618\n",
      "iteration 400 / 1000: loss 0.474423\n",
      "iteration 500 / 1000: loss 0.499001\n",
      "iteration 600 / 1000: loss 0.469582\n",
      "iteration 700 / 1000: loss 0.452341\n",
      "iteration 800 / 1000: loss 0.463164\n",
      "iteration 900 / 1000: loss 0.454380\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099430\n",
      "iteration 100 / 1000: loss 0.450008\n",
      "iteration 200 / 1000: loss 0.460917\n",
      "iteration 300 / 1000: loss 0.453647\n",
      "iteration 400 / 1000: loss 0.498849\n",
      "iteration 500 / 1000: loss 0.474164\n",
      "iteration 600 / 1000: loss 0.470276\n",
      "iteration 700 / 1000: loss 0.488486\n",
      "iteration 800 / 1000: loss 0.517589\n",
      "iteration 900 / 1000: loss 0.462031\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097761\n",
      "iteration 100 / 1000: loss 0.450026\n",
      "iteration 200 / 1000: loss 0.461239\n",
      "iteration 300 / 1000: loss 0.514205\n",
      "iteration 400 / 1000: loss 0.465144\n",
      "iteration 500 / 1000: loss 0.477040\n",
      "iteration 600 / 1000: loss 0.466097\n",
      "iteration 700 / 1000: loss 0.462468\n",
      "iteration 800 / 1000: loss 0.464970\n",
      "iteration 900 / 1000: loss 0.498924\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098980\n",
      "iteration 100 / 1000: loss 0.469038\n",
      "iteration 200 / 1000: loss 0.513728\n",
      "iteration 300 / 1000: loss 0.457101\n",
      "iteration 400 / 1000: loss 0.487944\n",
      "iteration 500 / 1000: loss 0.506148\n",
      "iteration 600 / 1000: loss 0.488524\n",
      "iteration 700 / 1000: loss 0.463403\n",
      "iteration 800 / 1000: loss 0.540278\n",
      "iteration 900 / 1000: loss 0.457144\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099468\n",
      "iteration 100 / 1000: loss 0.439380\n",
      "iteration 200 / 1000: loss 0.397041\n",
      "iteration 300 / 1000: loss 0.412325\n",
      "iteration 400 / 1000: loss 0.438426\n",
      "iteration 500 / 1000: loss 0.395359\n",
      "iteration 600 / 1000: loss 0.447251\n",
      "iteration 700 / 1000: loss 0.387261\n",
      "iteration 800 / 1000: loss 0.378727\n",
      "iteration 900 / 1000: loss 0.446332\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098362\n",
      "iteration 100 / 1000: loss 0.447172\n",
      "iteration 200 / 1000: loss 0.402534\n",
      "iteration 300 / 1000: loss 0.477745\n",
      "iteration 400 / 1000: loss 0.445448\n",
      "iteration 500 / 1000: loss 0.390322\n",
      "iteration 600 / 1000: loss 0.450395\n",
      "iteration 700 / 1000: loss 0.426195\n",
      "iteration 800 / 1000: loss 0.482226\n",
      "iteration 900 / 1000: loss 0.377525\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098356\n",
      "iteration 100 / 1000: loss 0.428454\n",
      "iteration 200 / 1000: loss 0.417991\n",
      "iteration 300 / 1000: loss 0.414234\n",
      "iteration 400 / 1000: loss 0.410634\n",
      "iteration 500 / 1000: loss 0.435227\n",
      "iteration 600 / 1000: loss 0.460406\n",
      "iteration 700 / 1000: loss 0.430905\n",
      "iteration 800 / 1000: loss 0.418711\n",
      "iteration 900 / 1000: loss 0.434854\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099358\n",
      "iteration 100 / 1000: loss 0.426240\n",
      "iteration 200 / 1000: loss 0.426905\n",
      "iteration 300 / 1000: loss 0.422105\n",
      "iteration 400 / 1000: loss 0.433671\n",
      "iteration 500 / 1000: loss 0.464429\n",
      "iteration 600 / 1000: loss 0.444580\n",
      "iteration 700 / 1000: loss 0.458753\n",
      "iteration 800 / 1000: loss 0.452468\n",
      "iteration 900 / 1000: loss 0.481452\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098997\n",
      "iteration 100 / 1000: loss 0.443866\n",
      "iteration 200 / 1000: loss 0.401752\n",
      "iteration 300 / 1000: loss 0.469974\n",
      "iteration 400 / 1000: loss 0.450710\n",
      "iteration 500 / 1000: loss 0.520680\n",
      "iteration 600 / 1000: loss 0.489723\n",
      "iteration 700 / 1000: loss 0.475929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.416748\n",
      "iteration 900 / 1000: loss 0.470814\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099198\n",
      "iteration 100 / 1000: loss 0.432337\n",
      "iteration 200 / 1000: loss 0.512450\n",
      "iteration 300 / 1000: loss 0.396821\n",
      "iteration 400 / 1000: loss 0.406298\n",
      "iteration 500 / 1000: loss 0.524833\n",
      "iteration 600 / 1000: loss 0.476213\n",
      "iteration 700 / 1000: loss 0.446625\n",
      "iteration 800 / 1000: loss 0.459009\n",
      "iteration 900 / 1000: loss 0.405203\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100007\n",
      "iteration 100 / 1000: loss 0.436286\n",
      "iteration 200 / 1000: loss 0.443007\n",
      "iteration 300 / 1000: loss 0.423321\n",
      "iteration 400 / 1000: loss 0.434938\n",
      "iteration 500 / 1000: loss 0.514439\n",
      "iteration 600 / 1000: loss 0.488032\n",
      "iteration 700 / 1000: loss 0.445446\n",
      "iteration 800 / 1000: loss 0.467576\n",
      "iteration 900 / 1000: loss 0.482914\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097672\n",
      "iteration 100 / 1000: loss 0.491217\n",
      "iteration 200 / 1000: loss 0.467392\n",
      "iteration 300 / 1000: loss 0.470460\n",
      "iteration 400 / 1000: loss 0.496820\n",
      "iteration 500 / 1000: loss 0.455954\n",
      "iteration 600 / 1000: loss 0.491344\n",
      "iteration 700 / 1000: loss 0.414429\n",
      "iteration 800 / 1000: loss 0.415413\n",
      "iteration 900 / 1000: loss 0.494438\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096664\n",
      "iteration 100 / 1000: loss 0.457051\n",
      "iteration 200 / 1000: loss 0.476318\n",
      "iteration 300 / 1000: loss 0.420411\n",
      "iteration 400 / 1000: loss 0.463507\n",
      "iteration 500 / 1000: loss 0.471317\n",
      "iteration 600 / 1000: loss 0.489305\n",
      "iteration 700 / 1000: loss 0.456260\n",
      "iteration 800 / 1000: loss 0.451646\n",
      "iteration 900 / 1000: loss 0.456087\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097760\n",
      "iteration 100 / 1000: loss 0.481063\n",
      "iteration 200 / 1000: loss 0.475273\n",
      "iteration 300 / 1000: loss 0.512831\n",
      "iteration 400 / 1000: loss 0.473154\n",
      "iteration 500 / 1000: loss 0.468376\n",
      "iteration 600 / 1000: loss 0.468656\n",
      "iteration 700 / 1000: loss 0.510358\n",
      "iteration 800 / 1000: loss 0.468205\n",
      "iteration 900 / 1000: loss 0.501482\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098865\n",
      "iteration 100 / 1000: loss 0.457483\n",
      "iteration 200 / 1000: loss 0.421014\n",
      "iteration 300 / 1000: loss 0.480545\n",
      "iteration 400 / 1000: loss 0.455262\n",
      "iteration 500 / 1000: loss 0.446162\n",
      "iteration 600 / 1000: loss 0.492370\n",
      "iteration 700 / 1000: loss 0.472572\n",
      "iteration 800 / 1000: loss 0.478650\n",
      "iteration 900 / 1000: loss 0.492060\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100967\n",
      "iteration 100 / 1000: loss 0.482079\n",
      "iteration 200 / 1000: loss 0.481752\n",
      "iteration 300 / 1000: loss 0.426992\n",
      "iteration 400 / 1000: loss 0.491963\n",
      "iteration 500 / 1000: loss 0.490892\n",
      "iteration 600 / 1000: loss 0.493472\n",
      "iteration 700 / 1000: loss 0.474372\n",
      "iteration 800 / 1000: loss 0.419983\n",
      "iteration 900 / 1000: loss 0.489623\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097443\n",
      "iteration 100 / 1000: loss 0.563335\n",
      "iteration 200 / 1000: loss 0.494373\n",
      "iteration 300 / 1000: loss 0.429143\n",
      "iteration 400 / 1000: loss 0.457260\n",
      "iteration 500 / 1000: loss 0.502102\n",
      "iteration 600 / 1000: loss 0.522763\n",
      "iteration 700 / 1000: loss 0.513800\n",
      "iteration 800 / 1000: loss 0.496290\n",
      "iteration 900 / 1000: loss 0.465049\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098334\n",
      "iteration 100 / 1000: loss 0.400930\n",
      "iteration 200 / 1000: loss 0.436819\n",
      "iteration 300 / 1000: loss 0.396516\n",
      "iteration 400 / 1000: loss 0.455209\n",
      "iteration 500 / 1000: loss 0.466656\n",
      "iteration 600 / 1000: loss 0.378675\n",
      "iteration 700 / 1000: loss 0.435963\n",
      "iteration 800 / 1000: loss 0.405743\n",
      "iteration 900 / 1000: loss 0.394727\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098123\n",
      "iteration 100 / 1000: loss 0.460892\n",
      "iteration 200 / 1000: loss 0.388919\n",
      "iteration 300 / 1000: loss 0.491826\n",
      "iteration 400 / 1000: loss 0.442622\n",
      "iteration 500 / 1000: loss 0.407336\n",
      "iteration 600 / 1000: loss 0.398694\n",
      "iteration 700 / 1000: loss 0.462784\n",
      "iteration 800 / 1000: loss 0.421462\n",
      "iteration 900 / 1000: loss 0.444059\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097747\n",
      "iteration 100 / 1000: loss 0.433253\n",
      "iteration 200 / 1000: loss 0.410456\n",
      "iteration 300 / 1000: loss 0.423292\n",
      "iteration 400 / 1000: loss 0.429471\n",
      "iteration 500 / 1000: loss 0.456476\n",
      "iteration 600 / 1000: loss 0.428004\n",
      "iteration 700 / 1000: loss 0.437565\n",
      "iteration 800 / 1000: loss 0.476726\n",
      "iteration 900 / 1000: loss 0.387379\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097953\n",
      "iteration 100 / 1000: loss 0.447408\n",
      "iteration 200 / 1000: loss 0.394285\n",
      "iteration 300 / 1000: loss 0.469300\n",
      "iteration 400 / 1000: loss 0.432246\n",
      "iteration 500 / 1000: loss 0.430943\n",
      "iteration 600 / 1000: loss 0.445091\n",
      "iteration 700 / 1000: loss 0.436287\n",
      "iteration 800 / 1000: loss 0.412975\n",
      "iteration 900 / 1000: loss 0.456439\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097656\n",
      "iteration 100 / 1000: loss 0.507144\n",
      "iteration 200 / 1000: loss 0.429556\n",
      "iteration 300 / 1000: loss 0.492146\n",
      "iteration 400 / 1000: loss 0.409242\n",
      "iteration 500 / 1000: loss 0.439594\n",
      "iteration 600 / 1000: loss 0.433358\n",
      "iteration 700 / 1000: loss 0.435430\n",
      "iteration 800 / 1000: loss 0.439238\n",
      "iteration 900 / 1000: loss 0.457772\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097004\n",
      "iteration 100 / 1000: loss 0.428174\n",
      "iteration 200 / 1000: loss 0.501148\n",
      "iteration 300 / 1000: loss 0.429335\n",
      "iteration 400 / 1000: loss 0.548693\n",
      "iteration 500 / 1000: loss 0.452442\n",
      "iteration 600 / 1000: loss 0.452026\n",
      "iteration 700 / 1000: loss 0.426544\n",
      "iteration 800 / 1000: loss 0.458238\n",
      "iteration 900 / 1000: loss 0.506419\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097294\n",
      "iteration 100 / 1000: loss 0.489674\n",
      "iteration 200 / 1000: loss 0.448060\n",
      "iteration 300 / 1000: loss 0.428866\n",
      "iteration 400 / 1000: loss 0.443612\n",
      "iteration 500 / 1000: loss 0.423052\n",
      "iteration 600 / 1000: loss 0.497368\n",
      "iteration 700 / 1000: loss 0.426962\n",
      "iteration 800 / 1000: loss 0.490328\n",
      "iteration 900 / 1000: loss 0.446193\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097852\n",
      "iteration 100 / 1000: loss 0.456698\n",
      "iteration 200 / 1000: loss 0.449171\n",
      "iteration 300 / 1000: loss 0.427178\n",
      "iteration 400 / 1000: loss 0.480378\n",
      "iteration 500 / 1000: loss 0.464550\n",
      "iteration 600 / 1000: loss 0.427586\n",
      "iteration 700 / 1000: loss 0.483828\n",
      "iteration 800 / 1000: loss 0.491010\n",
      "iteration 900 / 1000: loss 0.443571\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099796\n",
      "iteration 100 / 1000: loss 0.415366\n",
      "iteration 200 / 1000: loss 0.415526\n",
      "iteration 300 / 1000: loss 0.400222\n",
      "iteration 400 / 1000: loss 0.494551\n",
      "iteration 500 / 1000: loss 0.496539\n",
      "iteration 600 / 1000: loss 0.452482\n",
      "iteration 700 / 1000: loss 0.457611\n",
      "iteration 800 / 1000: loss 0.470657\n",
      "iteration 900 / 1000: loss 0.460392\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096763\n",
      "iteration 100 / 1000: loss 0.478432\n",
      "iteration 200 / 1000: loss 0.448147\n",
      "iteration 300 / 1000: loss 0.455496\n",
      "iteration 400 / 1000: loss 0.500829\n",
      "iteration 500 / 1000: loss 0.445090\n",
      "iteration 600 / 1000: loss 0.462756\n",
      "iteration 700 / 1000: loss 0.480600\n",
      "iteration 800 / 1000: loss 0.454396\n",
      "iteration 900 / 1000: loss 0.462150\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095884\n",
      "iteration 100 / 1000: loss 0.488471\n",
      "iteration 200 / 1000: loss 0.458251\n",
      "iteration 300 / 1000: loss 0.467992\n",
      "iteration 400 / 1000: loss 0.445055\n",
      "iteration 500 / 1000: loss 0.485908\n",
      "iteration 600 / 1000: loss 0.480800\n",
      "iteration 700 / 1000: loss 0.434340\n",
      "iteration 800 / 1000: loss 0.465963\n",
      "iteration 900 / 1000: loss 0.476509\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098429\n",
      "iteration 100 / 1000: loss 0.426822\n",
      "iteration 200 / 1000: loss 0.456707\n",
      "iteration 300 / 1000: loss 0.524288\n",
      "iteration 400 / 1000: loss 0.443813\n",
      "iteration 500 / 1000: loss 0.476531\n",
      "iteration 600 / 1000: loss 0.454184\n",
      "iteration 700 / 1000: loss 0.490209\n",
      "iteration 800 / 1000: loss 0.477684\n",
      "iteration 900 / 1000: loss 0.491426\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097440\n",
      "iteration 100 / 1000: loss 0.459483\n",
      "iteration 200 / 1000: loss 0.464470\n",
      "iteration 300 / 1000: loss 0.439220\n",
      "iteration 400 / 1000: loss 0.478744\n",
      "iteration 500 / 1000: loss 0.452204\n",
      "iteration 600 / 1000: loss 0.482367\n",
      "iteration 700 / 1000: loss 0.503651\n",
      "iteration 800 / 1000: loss 0.486525\n",
      "iteration 900 / 1000: loss 0.474562\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097958\n",
      "iteration 100 / 1000: loss 0.412654\n",
      "iteration 200 / 1000: loss 0.408909\n",
      "iteration 300 / 1000: loss 0.438640\n",
      "iteration 400 / 1000: loss 0.411632\n",
      "iteration 500 / 1000: loss 0.443767\n",
      "iteration 600 / 1000: loss 0.446050\n",
      "iteration 700 / 1000: loss 0.421617\n",
      "iteration 800 / 1000: loss 0.412207\n",
      "iteration 900 / 1000: loss 0.402320\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.101520\n",
      "iteration 100 / 1000: loss 0.447180\n",
      "iteration 200 / 1000: loss 0.434190\n",
      "iteration 300 / 1000: loss 0.404655\n",
      "iteration 400 / 1000: loss 0.437007\n",
      "iteration 500 / 1000: loss 0.428964\n",
      "iteration 600 / 1000: loss 0.431638\n",
      "iteration 700 / 1000: loss 0.413622\n",
      "iteration 800 / 1000: loss 0.379051\n",
      "iteration 900 / 1000: loss 0.496140\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097370\n",
      "iteration 100 / 1000: loss 0.430493\n",
      "iteration 200 / 1000: loss 0.438235\n",
      "iteration 300 / 1000: loss 0.420933\n",
      "iteration 400 / 1000: loss 0.477763\n",
      "iteration 500 / 1000: loss 0.436377\n",
      "iteration 600 / 1000: loss 0.399989\n",
      "iteration 700 / 1000: loss 0.427764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.456262\n",
      "iteration 900 / 1000: loss 0.441512\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099817\n",
      "iteration 100 / 1000: loss 0.441284\n",
      "iteration 200 / 1000: loss 0.450390\n",
      "iteration 300 / 1000: loss 0.466519\n",
      "iteration 400 / 1000: loss 0.430992\n",
      "iteration 500 / 1000: loss 0.449320\n",
      "iteration 600 / 1000: loss 0.461108\n",
      "iteration 700 / 1000: loss 0.416402\n",
      "iteration 800 / 1000: loss 0.399062\n",
      "iteration 900 / 1000: loss 0.453182\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098079\n",
      "iteration 100 / 1000: loss 0.468701\n",
      "iteration 200 / 1000: loss 0.477434\n",
      "iteration 300 / 1000: loss 0.437518\n",
      "iteration 400 / 1000: loss 0.456128\n",
      "iteration 500 / 1000: loss 0.509031\n",
      "iteration 600 / 1000: loss 0.475745\n",
      "iteration 700 / 1000: loss 0.416760\n",
      "iteration 800 / 1000: loss 0.425345\n",
      "iteration 900 / 1000: loss 0.475350\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097831\n",
      "iteration 100 / 1000: loss 0.457335\n",
      "iteration 200 / 1000: loss 0.493281\n",
      "iteration 300 / 1000: loss 0.425814\n",
      "iteration 400 / 1000: loss 0.479651\n",
      "iteration 500 / 1000: loss 0.470077\n",
      "iteration 600 / 1000: loss 0.390210\n",
      "iteration 700 / 1000: loss 0.417727\n",
      "iteration 800 / 1000: loss 0.480323\n",
      "iteration 900 / 1000: loss 0.504966\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096428\n",
      "iteration 100 / 1000: loss 0.434478\n",
      "iteration 200 / 1000: loss 0.399851\n",
      "iteration 300 / 1000: loss 0.453424\n",
      "iteration 400 / 1000: loss 0.461257\n",
      "iteration 500 / 1000: loss 0.445420\n",
      "iteration 600 / 1000: loss 0.408763\n",
      "iteration 700 / 1000: loss 0.455536\n",
      "iteration 800 / 1000: loss 0.470193\n",
      "iteration 900 / 1000: loss 0.466920\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097651\n",
      "iteration 100 / 1000: loss 0.443226\n",
      "iteration 200 / 1000: loss 0.498667\n",
      "iteration 300 / 1000: loss 0.398746\n",
      "iteration 400 / 1000: loss 0.508942\n",
      "iteration 500 / 1000: loss 0.478513\n",
      "iteration 600 / 1000: loss 0.476695\n",
      "iteration 700 / 1000: loss 0.503816\n",
      "iteration 800 / 1000: loss 0.444472\n",
      "iteration 900 / 1000: loss 0.473048\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099430\n",
      "iteration 100 / 1000: loss 0.478274\n",
      "iteration 200 / 1000: loss 0.436424\n",
      "iteration 300 / 1000: loss 0.439300\n",
      "iteration 400 / 1000: loss 0.505686\n",
      "iteration 500 / 1000: loss 0.490395\n",
      "iteration 600 / 1000: loss 0.471034\n",
      "iteration 700 / 1000: loss 0.459823\n",
      "iteration 800 / 1000: loss 0.479862\n",
      "iteration 900 / 1000: loss 0.514845\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099360\n",
      "iteration 100 / 1000: loss 0.477703\n",
      "iteration 200 / 1000: loss 0.477082\n",
      "iteration 300 / 1000: loss 0.477580\n",
      "iteration 400 / 1000: loss 0.481440\n",
      "iteration 500 / 1000: loss 0.475321\n",
      "iteration 600 / 1000: loss 0.432977\n",
      "iteration 700 / 1000: loss 0.463989\n",
      "iteration 800 / 1000: loss 0.508171\n",
      "iteration 900 / 1000: loss 0.517321\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098804\n",
      "iteration 100 / 1000: loss 0.466962\n",
      "iteration 200 / 1000: loss 0.460335\n",
      "iteration 300 / 1000: loss 0.466471\n",
      "iteration 400 / 1000: loss 0.488748\n",
      "iteration 500 / 1000: loss 0.453204\n",
      "iteration 600 / 1000: loss 0.462508\n",
      "iteration 700 / 1000: loss 0.451828\n",
      "iteration 800 / 1000: loss 0.449098\n",
      "iteration 900 / 1000: loss 0.548638\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097874\n",
      "iteration 100 / 1000: loss 0.484764\n",
      "iteration 200 / 1000: loss 0.452809\n",
      "iteration 300 / 1000: loss 0.480154\n",
      "iteration 400 / 1000: loss 0.491953\n",
      "iteration 500 / 1000: loss 0.501971\n",
      "iteration 600 / 1000: loss 0.509335\n",
      "iteration 700 / 1000: loss 0.470811\n",
      "iteration 800 / 1000: loss 0.495737\n",
      "iteration 900 / 1000: loss 0.474333\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096434\n",
      "iteration 100 / 1000: loss 0.526061\n",
      "iteration 200 / 1000: loss 0.482902\n",
      "iteration 300 / 1000: loss 0.452133\n",
      "iteration 400 / 1000: loss 0.478166\n",
      "iteration 500 / 1000: loss 0.500217\n",
      "iteration 600 / 1000: loss 0.456432\n",
      "iteration 700 / 1000: loss 0.446586\n",
      "iteration 800 / 1000: loss 0.480331\n",
      "iteration 900 / 1000: loss 0.494808\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099185\n",
      "iteration 100 / 1000: loss 0.419874\n",
      "iteration 200 / 1000: loss 0.392535\n",
      "iteration 300 / 1000: loss 0.433909\n",
      "iteration 400 / 1000: loss 0.428080\n",
      "iteration 500 / 1000: loss 0.428791\n",
      "iteration 600 / 1000: loss 0.437549\n",
      "iteration 700 / 1000: loss 0.413381\n",
      "iteration 800 / 1000: loss 0.372498\n",
      "iteration 900 / 1000: loss 0.390132\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096153\n",
      "iteration 100 / 1000: loss 0.377137\n",
      "iteration 200 / 1000: loss 0.447938\n",
      "iteration 300 / 1000: loss 0.480660\n",
      "iteration 400 / 1000: loss 0.453556\n",
      "iteration 500 / 1000: loss 0.408402\n",
      "iteration 600 / 1000: loss 0.456171\n",
      "iteration 700 / 1000: loss 0.435906\n",
      "iteration 800 / 1000: loss 0.460369\n",
      "iteration 900 / 1000: loss 0.445517\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097039\n",
      "iteration 100 / 1000: loss 0.439051\n",
      "iteration 200 / 1000: loss 0.423483\n",
      "iteration 300 / 1000: loss 0.438807\n",
      "iteration 400 / 1000: loss 0.407519\n",
      "iteration 500 / 1000: loss 0.393422\n",
      "iteration 600 / 1000: loss 0.464831\n",
      "iteration 700 / 1000: loss 0.458264\n",
      "iteration 800 / 1000: loss 0.446726\n",
      "iteration 900 / 1000: loss 0.455815\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098545\n",
      "iteration 100 / 1000: loss 0.442468\n",
      "iteration 200 / 1000: loss 0.457428\n",
      "iteration 300 / 1000: loss 0.456676\n",
      "iteration 400 / 1000: loss 0.411881\n",
      "iteration 500 / 1000: loss 0.467499\n",
      "iteration 600 / 1000: loss 0.428175\n",
      "iteration 700 / 1000: loss 0.456370\n",
      "iteration 800 / 1000: loss 0.432891\n",
      "iteration 900 / 1000: loss 0.426178\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096688\n",
      "iteration 100 / 1000: loss 0.443575\n",
      "iteration 200 / 1000: loss 0.427293\n",
      "iteration 300 / 1000: loss 0.456482\n",
      "iteration 400 / 1000: loss 0.447881\n",
      "iteration 500 / 1000: loss 0.427900\n",
      "iteration 600 / 1000: loss 0.449871\n",
      "iteration 700 / 1000: loss 0.494990\n",
      "iteration 800 / 1000: loss 0.462853\n",
      "iteration 900 / 1000: loss 0.402476\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098266\n",
      "iteration 100 / 1000: loss 0.467684\n",
      "iteration 200 / 1000: loss 0.447039\n",
      "iteration 300 / 1000: loss 0.443829\n",
      "iteration 400 / 1000: loss 0.434060\n",
      "iteration 500 / 1000: loss 0.396224\n",
      "iteration 600 / 1000: loss 0.433611\n",
      "iteration 700 / 1000: loss 0.511436\n",
      "iteration 800 / 1000: loss 0.497832\n",
      "iteration 900 / 1000: loss 0.454929\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097278\n",
      "iteration 100 / 1000: loss 0.456747\n",
      "iteration 200 / 1000: loss 0.456059\n",
      "iteration 300 / 1000: loss 0.483040\n",
      "iteration 400 / 1000: loss 0.503825\n",
      "iteration 500 / 1000: loss 0.433233\n",
      "iteration 600 / 1000: loss 0.435470\n",
      "iteration 700 / 1000: loss 0.448574\n",
      "iteration 800 / 1000: loss 0.448048\n",
      "iteration 900 / 1000: loss 0.476504\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098803\n",
      "iteration 100 / 1000: loss 0.429786\n",
      "iteration 200 / 1000: loss 0.441021\n",
      "iteration 300 / 1000: loss 0.446571\n",
      "iteration 400 / 1000: loss 0.466958\n",
      "iteration 500 / 1000: loss 0.478849\n",
      "iteration 600 / 1000: loss 0.483197\n",
      "iteration 700 / 1000: loss 0.474521\n",
      "iteration 800 / 1000: loss 0.444046\n",
      "iteration 900 / 1000: loss 0.429307\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098606\n",
      "iteration 100 / 1000: loss 0.460295\n",
      "iteration 200 / 1000: loss 0.548149\n",
      "iteration 300 / 1000: loss 0.467028\n",
      "iteration 400 / 1000: loss 0.507250\n",
      "iteration 500 / 1000: loss 0.481946\n",
      "iteration 600 / 1000: loss 0.483277\n",
      "iteration 700 / 1000: loss 0.506001\n",
      "iteration 800 / 1000: loss 0.442196\n",
      "iteration 900 / 1000: loss 0.438400\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099493\n",
      "iteration 100 / 1000: loss 0.512123\n",
      "iteration 200 / 1000: loss 0.482096\n",
      "iteration 300 / 1000: loss 0.470863\n",
      "iteration 400 / 1000: loss 0.423162\n",
      "iteration 500 / 1000: loss 0.426359\n",
      "iteration 600 / 1000: loss 0.490446\n",
      "iteration 700 / 1000: loss 0.486497\n",
      "iteration 800 / 1000: loss 0.443695\n",
      "iteration 900 / 1000: loss 0.472467\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096315\n",
      "iteration 100 / 1000: loss 0.475007\n",
      "iteration 200 / 1000: loss 0.437179\n",
      "iteration 300 / 1000: loss 0.462496\n",
      "iteration 400 / 1000: loss 0.437095\n",
      "iteration 500 / 1000: loss 0.437433\n",
      "iteration 600 / 1000: loss 0.447981\n",
      "iteration 700 / 1000: loss 0.452019\n",
      "iteration 800 / 1000: loss 0.482362\n",
      "iteration 900 / 1000: loss 0.476451\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100180\n",
      "iteration 100 / 1000: loss 0.511353\n",
      "iteration 200 / 1000: loss 0.470828\n",
      "iteration 300 / 1000: loss 0.441086\n",
      "iteration 400 / 1000: loss 0.469593\n",
      "iteration 500 / 1000: loss 0.515195\n",
      "iteration 600 / 1000: loss 0.478758\n",
      "iteration 700 / 1000: loss 0.453498\n",
      "iteration 800 / 1000: loss 0.510408\n",
      "iteration 900 / 1000: loss 0.509882\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098081\n",
      "iteration 100 / 1000: loss 0.467048\n",
      "iteration 200 / 1000: loss 0.449518\n",
      "iteration 300 / 1000: loss 0.497076\n",
      "iteration 400 / 1000: loss 0.435089\n",
      "iteration 500 / 1000: loss 0.478939\n",
      "iteration 600 / 1000: loss 0.520921\n",
      "iteration 700 / 1000: loss 0.459492\n",
      "iteration 800 / 1000: loss 0.406210\n",
      "iteration 900 / 1000: loss 0.529584\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097135\n",
      "iteration 100 / 1000: loss 0.394707\n",
      "iteration 200 / 1000: loss 0.435481\n",
      "iteration 300 / 1000: loss 0.443952\n",
      "iteration 400 / 1000: loss 0.460794\n",
      "iteration 500 / 1000: loss 0.445473\n",
      "iteration 600 / 1000: loss 0.425901\n",
      "iteration 700 / 1000: loss 0.396994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.438534\n",
      "iteration 900 / 1000: loss 0.433392\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.101764\n",
      "iteration 100 / 1000: loss 0.464485\n",
      "iteration 200 / 1000: loss 0.450346\n",
      "iteration 300 / 1000: loss 0.461685\n",
      "iteration 400 / 1000: loss 0.500119\n",
      "iteration 500 / 1000: loss 0.412667\n",
      "iteration 600 / 1000: loss 0.477722\n",
      "iteration 700 / 1000: loss 0.429528\n",
      "iteration 800 / 1000: loss 0.466324\n",
      "iteration 900 / 1000: loss 0.471485\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098597\n",
      "iteration 100 / 1000: loss 0.452739\n",
      "iteration 200 / 1000: loss 0.478948\n",
      "iteration 300 / 1000: loss 0.403828\n",
      "iteration 400 / 1000: loss 0.421309\n",
      "iteration 500 / 1000: loss 0.426196\n",
      "iteration 600 / 1000: loss 0.463631\n",
      "iteration 700 / 1000: loss 0.443222\n",
      "iteration 800 / 1000: loss 0.400952\n",
      "iteration 900 / 1000: loss 0.428507\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100303\n",
      "iteration 100 / 1000: loss 0.472524\n",
      "iteration 200 / 1000: loss 0.404125\n",
      "iteration 300 / 1000: loss 0.408880\n",
      "iteration 400 / 1000: loss 0.428868\n",
      "iteration 500 / 1000: loss 0.472775\n",
      "iteration 600 / 1000: loss 0.432943\n",
      "iteration 700 / 1000: loss 0.464556\n",
      "iteration 800 / 1000: loss 0.439599\n",
      "iteration 900 / 1000: loss 0.422547\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098425\n",
      "iteration 100 / 1000: loss 0.445079\n",
      "iteration 200 / 1000: loss 0.398475\n",
      "iteration 300 / 1000: loss 0.472214\n",
      "iteration 400 / 1000: loss 0.469370\n",
      "iteration 500 / 1000: loss 0.488902\n",
      "iteration 600 / 1000: loss 0.495460\n",
      "iteration 700 / 1000: loss 0.438240\n",
      "iteration 800 / 1000: loss 0.420085\n",
      "iteration 900 / 1000: loss 0.481387\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099978\n",
      "iteration 100 / 1000: loss 0.490470\n",
      "iteration 200 / 1000: loss 0.427374\n",
      "iteration 300 / 1000: loss 0.434059\n",
      "iteration 400 / 1000: loss 0.419430\n",
      "iteration 500 / 1000: loss 0.439123\n",
      "iteration 600 / 1000: loss 0.411649\n",
      "iteration 700 / 1000: loss 0.462670\n",
      "iteration 800 / 1000: loss 0.479528\n",
      "iteration 900 / 1000: loss 0.449900\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099050\n",
      "iteration 100 / 1000: loss 0.488003\n",
      "iteration 200 / 1000: loss 0.459594\n",
      "iteration 300 / 1000: loss 0.435555\n",
      "iteration 400 / 1000: loss 0.450283\n",
      "iteration 500 / 1000: loss 0.466000\n",
      "iteration 600 / 1000: loss 0.502877\n",
      "iteration 700 / 1000: loss 0.463080\n",
      "iteration 800 / 1000: loss 0.447045\n",
      "iteration 900 / 1000: loss 0.438770\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099360\n",
      "iteration 100 / 1000: loss 0.443400\n",
      "iteration 200 / 1000: loss 0.449684\n",
      "iteration 300 / 1000: loss 0.425749\n",
      "iteration 400 / 1000: loss 0.445714\n",
      "iteration 500 / 1000: loss 0.465084\n",
      "iteration 600 / 1000: loss 0.434644\n",
      "iteration 700 / 1000: loss 0.428378\n",
      "iteration 800 / 1000: loss 0.439519\n",
      "iteration 900 / 1000: loss 0.440301\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096417\n",
      "iteration 100 / 1000: loss 0.404385\n",
      "iteration 200 / 1000: loss 0.471173\n",
      "iteration 300 / 1000: loss 0.422277\n",
      "iteration 400 / 1000: loss 0.453168\n",
      "iteration 500 / 1000: loss 0.443197\n",
      "iteration 600 / 1000: loss 0.500046\n",
      "iteration 700 / 1000: loss 0.473807\n",
      "iteration 800 / 1000: loss 0.463908\n",
      "iteration 900 / 1000: loss 0.463423\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097778\n",
      "iteration 100 / 1000: loss 0.476550\n",
      "iteration 200 / 1000: loss 0.443372\n",
      "iteration 300 / 1000: loss 0.422233\n",
      "iteration 400 / 1000: loss 0.441225\n",
      "iteration 500 / 1000: loss 0.456457\n",
      "iteration 600 / 1000: loss 0.492123\n",
      "iteration 700 / 1000: loss 0.434531\n",
      "iteration 800 / 1000: loss 0.442452\n",
      "iteration 900 / 1000: loss 0.455927\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097665\n",
      "iteration 100 / 1000: loss 0.466395\n",
      "iteration 200 / 1000: loss 0.442252\n",
      "iteration 300 / 1000: loss 0.453865\n",
      "iteration 400 / 1000: loss 0.496090\n",
      "iteration 500 / 1000: loss 0.469683\n",
      "iteration 600 / 1000: loss 0.494980\n",
      "iteration 700 / 1000: loss 0.452144\n",
      "iteration 800 / 1000: loss 0.480149\n",
      "iteration 900 / 1000: loss 0.430043\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098471\n",
      "iteration 100 / 1000: loss 0.415174\n",
      "iteration 200 / 1000: loss 0.478104\n",
      "iteration 300 / 1000: loss 0.486417\n",
      "iteration 400 / 1000: loss 0.484565\n",
      "iteration 500 / 1000: loss 0.473583\n",
      "iteration 600 / 1000: loss 0.462187\n",
      "iteration 700 / 1000: loss 0.462363\n",
      "iteration 800 / 1000: loss 0.475959\n",
      "iteration 900 / 1000: loss 0.476799\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098317\n",
      "iteration 100 / 1000: loss 0.482643\n",
      "iteration 200 / 1000: loss 0.480945\n",
      "iteration 300 / 1000: loss 0.498850\n",
      "iteration 400 / 1000: loss 0.470913\n",
      "iteration 500 / 1000: loss 0.465645\n",
      "iteration 600 / 1000: loss 0.446497\n",
      "iteration 700 / 1000: loss 0.462179\n",
      "iteration 800 / 1000: loss 0.458173\n",
      "iteration 900 / 1000: loss 0.482244\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099268\n",
      "iteration 100 / 1000: loss 0.460973\n",
      "iteration 200 / 1000: loss 0.352368\n",
      "iteration 300 / 1000: loss 0.407254\n",
      "iteration 400 / 1000: loss 0.466345\n",
      "iteration 500 / 1000: loss 0.398228\n",
      "iteration 600 / 1000: loss 0.428705\n",
      "iteration 700 / 1000: loss 0.435906\n",
      "iteration 800 / 1000: loss 0.427187\n",
      "iteration 900 / 1000: loss 0.418727\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.101608\n",
      "iteration 100 / 1000: loss 0.421651\n",
      "iteration 200 / 1000: loss 0.401332\n",
      "iteration 300 / 1000: loss 0.392361\n",
      "iteration 400 / 1000: loss 0.424477\n",
      "iteration 500 / 1000: loss 0.448699\n",
      "iteration 600 / 1000: loss 0.419066\n",
      "iteration 700 / 1000: loss 0.399882\n",
      "iteration 800 / 1000: loss 0.394720\n",
      "iteration 900 / 1000: loss 0.463303\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095455\n",
      "iteration 100 / 1000: loss 0.400294\n",
      "iteration 200 / 1000: loss 0.422304\n",
      "iteration 300 / 1000: loss 0.415686\n",
      "iteration 400 / 1000: loss 0.450102\n",
      "iteration 500 / 1000: loss 0.453062\n",
      "iteration 600 / 1000: loss 0.488255\n",
      "iteration 700 / 1000: loss 0.433977\n",
      "iteration 800 / 1000: loss 0.451557\n",
      "iteration 900 / 1000: loss 0.460079\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098873\n",
      "iteration 100 / 1000: loss 0.447905\n",
      "iteration 200 / 1000: loss 0.430638\n",
      "iteration 300 / 1000: loss 0.461430\n",
      "iteration 400 / 1000: loss 0.451083\n",
      "iteration 500 / 1000: loss 0.432726\n",
      "iteration 600 / 1000: loss 0.422551\n",
      "iteration 700 / 1000: loss 0.426251\n",
      "iteration 800 / 1000: loss 0.432860\n",
      "iteration 900 / 1000: loss 0.449389\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097878\n",
      "iteration 100 / 1000: loss 0.451608\n",
      "iteration 200 / 1000: loss 0.433856\n",
      "iteration 300 / 1000: loss 0.427151\n",
      "iteration 400 / 1000: loss 0.465237\n",
      "iteration 500 / 1000: loss 0.415360\n",
      "iteration 600 / 1000: loss 0.450186\n",
      "iteration 700 / 1000: loss 0.464266\n",
      "iteration 800 / 1000: loss 0.445119\n",
      "iteration 900 / 1000: loss 0.440736\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097953\n",
      "iteration 100 / 1000: loss 0.406964\n",
      "iteration 200 / 1000: loss 0.497834\n",
      "iteration 300 / 1000: loss 0.441564\n",
      "iteration 400 / 1000: loss 0.469872\n",
      "iteration 500 / 1000: loss 0.430483\n",
      "iteration 600 / 1000: loss 0.448035\n",
      "iteration 700 / 1000: loss 0.442175\n",
      "iteration 800 / 1000: loss 0.441227\n",
      "iteration 900 / 1000: loss 0.427443\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098029\n",
      "iteration 100 / 1000: loss 0.479925\n",
      "iteration 200 / 1000: loss 0.398431\n",
      "iteration 300 / 1000: loss 0.435134\n",
      "iteration 400 / 1000: loss 0.470279\n",
      "iteration 500 / 1000: loss 0.476668\n",
      "iteration 600 / 1000: loss 0.460982\n",
      "iteration 700 / 1000: loss 0.497474\n",
      "iteration 800 / 1000: loss 0.446284\n",
      "iteration 900 / 1000: loss 0.480713\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100546\n",
      "iteration 100 / 1000: loss 0.473920\n",
      "iteration 200 / 1000: loss 0.467835\n",
      "iteration 300 / 1000: loss 0.477015\n",
      "iteration 400 / 1000: loss 0.500688\n",
      "iteration 500 / 1000: loss 0.444064\n",
      "iteration 600 / 1000: loss 0.422418\n",
      "iteration 700 / 1000: loss 0.444316\n",
      "iteration 800 / 1000: loss 0.449201\n",
      "iteration 900 / 1000: loss 0.473228\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099601\n",
      "iteration 100 / 1000: loss 0.425161\n",
      "iteration 200 / 1000: loss 0.455208\n",
      "iteration 300 / 1000: loss 0.515994\n",
      "iteration 400 / 1000: loss 0.469973\n",
      "iteration 500 / 1000: loss 0.365713\n",
      "iteration 600 / 1000: loss 0.501209\n",
      "iteration 700 / 1000: loss 0.470502\n",
      "iteration 800 / 1000: loss 0.454847\n",
      "iteration 900 / 1000: loss 0.521836\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099608\n",
      "iteration 100 / 1000: loss 0.508012\n",
      "iteration 200 / 1000: loss 0.456568\n",
      "iteration 300 / 1000: loss 0.440288\n",
      "iteration 400 / 1000: loss 0.435006\n",
      "iteration 500 / 1000: loss 0.454558\n",
      "iteration 600 / 1000: loss 0.497036\n",
      "iteration 700 / 1000: loss 0.440257\n",
      "iteration 800 / 1000: loss 0.424123\n",
      "iteration 900 / 1000: loss 0.425286\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097840\n",
      "iteration 100 / 1000: loss 0.462851\n",
      "iteration 200 / 1000: loss 0.486454\n",
      "iteration 300 / 1000: loss 0.470059\n",
      "iteration 400 / 1000: loss 0.489542\n",
      "iteration 500 / 1000: loss 0.468691\n",
      "iteration 600 / 1000: loss 0.455896\n",
      "iteration 700 / 1000: loss 0.441309\n",
      "iteration 800 / 1000: loss 0.432059\n",
      "iteration 900 / 1000: loss 0.496650\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096331\n",
      "iteration 100 / 1000: loss 0.476978\n",
      "iteration 200 / 1000: loss 0.480710\n",
      "iteration 300 / 1000: loss 0.472117\n",
      "iteration 400 / 1000: loss 0.497113\n",
      "iteration 500 / 1000: loss 0.464637\n",
      "iteration 600 / 1000: loss 0.463993\n",
      "iteration 700 / 1000: loss 0.447734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.453954\n",
      "iteration 900 / 1000: loss 0.432566\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097845\n",
      "iteration 100 / 1000: loss 0.433439\n",
      "iteration 200 / 1000: loss 0.478925\n",
      "iteration 300 / 1000: loss 0.433072\n",
      "iteration 400 / 1000: loss 0.461150\n",
      "iteration 500 / 1000: loss 0.458369\n",
      "iteration 600 / 1000: loss 0.406428\n",
      "iteration 700 / 1000: loss 0.459526\n",
      "iteration 800 / 1000: loss 0.449440\n",
      "iteration 900 / 1000: loss 0.514344\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100513\n",
      "iteration 100 / 1000: loss 0.402827\n",
      "iteration 200 / 1000: loss 0.387943\n",
      "iteration 300 / 1000: loss 0.442029\n",
      "iteration 400 / 1000: loss 0.400592\n",
      "iteration 500 / 1000: loss 0.456369\n",
      "iteration 600 / 1000: loss 0.376325\n",
      "iteration 700 / 1000: loss 0.423261\n",
      "iteration 800 / 1000: loss 0.428639\n",
      "iteration 900 / 1000: loss 0.451492\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099541\n",
      "iteration 100 / 1000: loss 0.415800\n",
      "iteration 200 / 1000: loss 0.392098\n",
      "iteration 300 / 1000: loss 0.403096\n",
      "iteration 400 / 1000: loss 0.435427\n",
      "iteration 500 / 1000: loss 0.416096\n",
      "iteration 600 / 1000: loss 0.428052\n",
      "iteration 700 / 1000: loss 0.426089\n",
      "iteration 800 / 1000: loss 0.437665\n",
      "iteration 900 / 1000: loss 0.416053\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098375\n",
      "iteration 100 / 1000: loss 0.468261\n",
      "iteration 200 / 1000: loss 0.398336\n",
      "iteration 300 / 1000: loss 0.442352\n",
      "iteration 400 / 1000: loss 0.501743\n",
      "iteration 500 / 1000: loss 0.453282\n",
      "iteration 600 / 1000: loss 0.436021\n",
      "iteration 700 / 1000: loss 0.441464\n",
      "iteration 800 / 1000: loss 0.419892\n",
      "iteration 900 / 1000: loss 0.443330\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098205\n",
      "iteration 100 / 1000: loss 0.425472\n",
      "iteration 200 / 1000: loss 0.434881\n",
      "iteration 300 / 1000: loss 0.446640\n",
      "iteration 400 / 1000: loss 0.430570\n",
      "iteration 500 / 1000: loss 0.410783\n",
      "iteration 600 / 1000: loss 0.423828\n",
      "iteration 700 / 1000: loss 0.461581\n",
      "iteration 800 / 1000: loss 0.414169\n",
      "iteration 900 / 1000: loss 0.416265\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097251\n",
      "iteration 100 / 1000: loss 0.466891\n",
      "iteration 200 / 1000: loss 0.454768\n",
      "iteration 300 / 1000: loss 0.437130\n",
      "iteration 400 / 1000: loss 0.442836\n",
      "iteration 500 / 1000: loss 0.439760\n",
      "iteration 600 / 1000: loss 0.452789\n",
      "iteration 700 / 1000: loss 0.455102\n",
      "iteration 800 / 1000: loss 0.436877\n",
      "iteration 900 / 1000: loss 0.442239\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095263\n",
      "iteration 100 / 1000: loss 0.475368\n",
      "iteration 200 / 1000: loss 0.478917\n",
      "iteration 300 / 1000: loss 0.469870\n",
      "iteration 400 / 1000: loss 0.417482\n",
      "iteration 500 / 1000: loss 0.409875\n",
      "iteration 600 / 1000: loss 0.453908\n",
      "iteration 700 / 1000: loss 0.393707\n",
      "iteration 800 / 1000: loss 0.452432\n",
      "iteration 900 / 1000: loss 0.408839\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097622\n",
      "iteration 100 / 1000: loss 0.459940\n",
      "iteration 200 / 1000: loss 0.502748\n",
      "iteration 300 / 1000: loss 0.469182\n",
      "iteration 400 / 1000: loss 0.439716\n",
      "iteration 500 / 1000: loss 0.493254\n",
      "iteration 600 / 1000: loss 0.433779\n",
      "iteration 700 / 1000: loss 0.521399\n",
      "iteration 800 / 1000: loss 0.480607\n",
      "iteration 900 / 1000: loss 0.436097\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099051\n",
      "iteration 100 / 1000: loss 0.491584\n",
      "iteration 200 / 1000: loss 0.504364\n",
      "iteration 300 / 1000: loss 0.488569\n",
      "iteration 400 / 1000: loss 0.485200\n",
      "iteration 500 / 1000: loss 0.484835\n",
      "iteration 600 / 1000: loss 0.447177\n",
      "iteration 700 / 1000: loss 0.501192\n",
      "iteration 800 / 1000: loss 0.486995\n",
      "iteration 900 / 1000: loss 0.450034\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098406\n",
      "iteration 100 / 1000: loss 0.486192\n",
      "iteration 200 / 1000: loss 0.443596\n",
      "iteration 300 / 1000: loss 0.462518\n",
      "iteration 400 / 1000: loss 0.466653\n",
      "iteration 500 / 1000: loss 0.494432\n",
      "iteration 600 / 1000: loss 0.402729\n",
      "iteration 700 / 1000: loss 0.461112\n",
      "iteration 800 / 1000: loss 0.463272\n",
      "iteration 900 / 1000: loss 0.459085\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100608\n",
      "iteration 100 / 1000: loss 0.496615\n",
      "iteration 200 / 1000: loss 0.463197\n",
      "iteration 300 / 1000: loss 0.514159\n",
      "iteration 400 / 1000: loss 0.471178\n",
      "iteration 500 / 1000: loss 0.471601\n",
      "iteration 600 / 1000: loss 0.466928\n",
      "iteration 700 / 1000: loss 0.491086\n",
      "iteration 800 / 1000: loss 0.445983\n",
      "iteration 900 / 1000: loss 0.457662\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096391\n",
      "iteration 100 / 1000: loss 0.485250\n",
      "iteration 200 / 1000: loss 0.492149\n",
      "iteration 300 / 1000: loss 0.476908\n",
      "iteration 400 / 1000: loss 0.458056\n",
      "iteration 500 / 1000: loss 0.450138\n",
      "iteration 600 / 1000: loss 0.442435\n",
      "iteration 700 / 1000: loss 0.533644\n",
      "iteration 800 / 1000: loss 0.513672\n",
      "iteration 900 / 1000: loss 0.461258\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099290\n",
      "iteration 100 / 1000: loss 0.499614\n",
      "iteration 200 / 1000: loss 0.494275\n",
      "iteration 300 / 1000: loss 0.520083\n",
      "iteration 400 / 1000: loss 0.457373\n",
      "iteration 500 / 1000: loss 0.437791\n",
      "iteration 600 / 1000: loss 0.492571\n",
      "iteration 700 / 1000: loss 0.497258\n",
      "iteration 800 / 1000: loss 0.519945\n",
      "iteration 900 / 1000: loss 0.488675\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099045\n",
      "iteration 100 / 1000: loss 0.442426\n",
      "iteration 200 / 1000: loss 0.467630\n",
      "iteration 300 / 1000: loss 0.430180\n",
      "iteration 400 / 1000: loss 0.438697\n",
      "iteration 500 / 1000: loss 0.512718\n",
      "iteration 600 / 1000: loss 0.438840\n",
      "iteration 700 / 1000: loss 0.468250\n",
      "iteration 800 / 1000: loss 0.471391\n",
      "iteration 900 / 1000: loss 0.444271\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097904\n",
      "iteration 100 / 1000: loss 0.400052\n",
      "iteration 200 / 1000: loss 0.452832\n",
      "iteration 300 / 1000: loss 0.443304\n",
      "iteration 400 / 1000: loss 0.394295\n",
      "iteration 500 / 1000: loss 0.458806\n",
      "iteration 600 / 1000: loss 0.399197\n",
      "iteration 700 / 1000: loss 0.394126\n",
      "iteration 800 / 1000: loss 0.429432\n",
      "iteration 900 / 1000: loss 0.425476\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096917\n",
      "iteration 100 / 1000: loss 0.376319\n",
      "iteration 200 / 1000: loss 0.458035\n",
      "iteration 300 / 1000: loss 0.429292\n",
      "iteration 400 / 1000: loss 0.447054\n",
      "iteration 500 / 1000: loss 0.457854\n",
      "iteration 600 / 1000: loss 0.445527\n",
      "iteration 700 / 1000: loss 0.455664\n",
      "iteration 800 / 1000: loss 0.448516\n",
      "iteration 900 / 1000: loss 0.449597\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098984\n",
      "iteration 100 / 1000: loss 0.413358\n",
      "iteration 200 / 1000: loss 0.444517\n",
      "iteration 300 / 1000: loss 0.421144\n",
      "iteration 400 / 1000: loss 0.441113\n",
      "iteration 500 / 1000: loss 0.426793\n",
      "iteration 600 / 1000: loss 0.425435\n",
      "iteration 700 / 1000: loss 0.422065\n",
      "iteration 800 / 1000: loss 0.455206\n",
      "iteration 900 / 1000: loss 0.434903\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097708\n",
      "iteration 100 / 1000: loss 0.459897\n",
      "iteration 200 / 1000: loss 0.370015\n",
      "iteration 300 / 1000: loss 0.436600\n",
      "iteration 400 / 1000: loss 0.456173\n",
      "iteration 500 / 1000: loss 0.440570\n",
      "iteration 600 / 1000: loss 0.452266\n",
      "iteration 700 / 1000: loss 0.408212\n",
      "iteration 800 / 1000: loss 0.377469\n",
      "iteration 900 / 1000: loss 0.416602\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098096\n",
      "iteration 100 / 1000: loss 0.479314\n",
      "iteration 200 / 1000: loss 0.484257\n",
      "iteration 300 / 1000: loss 0.483775\n",
      "iteration 400 / 1000: loss 0.426031\n",
      "iteration 500 / 1000: loss 0.451872\n",
      "iteration 600 / 1000: loss 0.396534\n",
      "iteration 700 / 1000: loss 0.486681\n",
      "iteration 800 / 1000: loss 0.439671\n",
      "iteration 900 / 1000: loss 0.488503\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098990\n",
      "iteration 100 / 1000: loss 0.435739\n",
      "iteration 200 / 1000: loss 0.437956\n",
      "iteration 300 / 1000: loss 0.423411\n",
      "iteration 400 / 1000: loss 0.441417\n",
      "iteration 500 / 1000: loss 0.440202\n",
      "iteration 600 / 1000: loss 0.441666\n",
      "iteration 700 / 1000: loss 0.414905\n",
      "iteration 800 / 1000: loss 0.446477\n",
      "iteration 900 / 1000: loss 0.463244\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098810\n",
      "iteration 100 / 1000: loss 0.435654\n",
      "iteration 200 / 1000: loss 0.452434\n",
      "iteration 300 / 1000: loss 0.439879\n",
      "iteration 400 / 1000: loss 0.518770\n",
      "iteration 500 / 1000: loss 0.496306\n",
      "iteration 600 / 1000: loss 0.472450\n",
      "iteration 700 / 1000: loss 0.460314\n",
      "iteration 800 / 1000: loss 0.447540\n",
      "iteration 900 / 1000: loss 0.475625\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100906\n",
      "iteration 100 / 1000: loss 0.470250\n",
      "iteration 200 / 1000: loss 0.460377\n",
      "iteration 300 / 1000: loss 0.426703\n",
      "iteration 400 / 1000: loss 0.487657\n",
      "iteration 500 / 1000: loss 0.442422\n",
      "iteration 600 / 1000: loss 0.464110\n",
      "iteration 700 / 1000: loss 0.466470\n",
      "iteration 800 / 1000: loss 0.517904\n",
      "iteration 900 / 1000: loss 0.448176\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099520\n",
      "iteration 100 / 1000: loss 0.492699\n",
      "iteration 200 / 1000: loss 0.418851\n",
      "iteration 300 / 1000: loss 0.441491\n",
      "iteration 400 / 1000: loss 0.450334\n",
      "iteration 500 / 1000: loss 0.465281\n",
      "iteration 600 / 1000: loss 0.483234\n",
      "iteration 700 / 1000: loss 0.416758\n",
      "iteration 800 / 1000: loss 0.481494\n",
      "iteration 900 / 1000: loss 0.490213\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098047\n",
      "iteration 100 / 1000: loss 0.484464\n",
      "iteration 200 / 1000: loss 0.450454\n",
      "iteration 300 / 1000: loss 0.456311\n",
      "iteration 400 / 1000: loss 0.434003\n",
      "iteration 500 / 1000: loss 0.482395\n",
      "iteration 600 / 1000: loss 0.442516\n",
      "iteration 700 / 1000: loss 0.486293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.449774\n",
      "iteration 900 / 1000: loss 0.487456\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100471\n",
      "iteration 100 / 1000: loss 0.408117\n",
      "iteration 200 / 1000: loss 0.505627\n",
      "iteration 300 / 1000: loss 0.513979\n",
      "iteration 400 / 1000: loss 0.440590\n",
      "iteration 500 / 1000: loss 0.510090\n",
      "iteration 600 / 1000: loss 0.466253\n",
      "iteration 700 / 1000: loss 0.482019\n",
      "iteration 800 / 1000: loss 0.477467\n",
      "iteration 900 / 1000: loss 0.447754\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097719\n",
      "iteration 100 / 1000: loss 0.467456\n",
      "iteration 200 / 1000: loss 0.486565\n",
      "iteration 300 / 1000: loss 0.463623\n",
      "iteration 400 / 1000: loss 0.474630\n",
      "iteration 500 / 1000: loss 0.497704\n",
      "iteration 600 / 1000: loss 0.478923\n",
      "iteration 700 / 1000: loss 0.451210\n",
      "iteration 800 / 1000: loss 0.470404\n",
      "iteration 900 / 1000: loss 0.485215\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098652\n",
      "iteration 100 / 1000: loss 0.447262\n",
      "iteration 200 / 1000: loss 0.430463\n",
      "iteration 300 / 1000: loss 0.453385\n",
      "iteration 400 / 1000: loss 0.474081\n",
      "iteration 500 / 1000: loss 0.468502\n",
      "iteration 600 / 1000: loss 0.476355\n",
      "iteration 700 / 1000: loss 0.488650\n",
      "iteration 800 / 1000: loss 0.484742\n",
      "iteration 900 / 1000: loss 0.466305\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098026\n",
      "iteration 100 / 1000: loss 0.447638\n",
      "iteration 200 / 1000: loss 0.453610\n",
      "iteration 300 / 1000: loss 0.426360\n",
      "iteration 400 / 1000: loss 0.422206\n",
      "iteration 500 / 1000: loss 0.455025\n",
      "iteration 600 / 1000: loss 0.413104\n",
      "iteration 700 / 1000: loss 0.440190\n",
      "iteration 800 / 1000: loss 0.352910\n",
      "iteration 900 / 1000: loss 0.377306\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099087\n",
      "iteration 100 / 1000: loss 0.399475\n",
      "iteration 200 / 1000: loss 0.463714\n",
      "iteration 300 / 1000: loss 0.424280\n",
      "iteration 400 / 1000: loss 0.431889\n",
      "iteration 500 / 1000: loss 0.445799\n",
      "iteration 600 / 1000: loss 0.435201\n",
      "iteration 700 / 1000: loss 0.493306\n",
      "iteration 800 / 1000: loss 0.448568\n",
      "iteration 900 / 1000: loss 0.411191\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095801\n",
      "iteration 100 / 1000: loss 0.448332\n",
      "iteration 200 / 1000: loss 0.440956\n",
      "iteration 300 / 1000: loss 0.486807\n",
      "iteration 400 / 1000: loss 0.427744\n",
      "iteration 500 / 1000: loss 0.408205\n",
      "iteration 600 / 1000: loss 0.495876\n",
      "iteration 700 / 1000: loss 0.434554\n",
      "iteration 800 / 1000: loss 0.460297\n",
      "iteration 900 / 1000: loss 0.469268\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098713\n",
      "iteration 100 / 1000: loss 0.447543\n",
      "iteration 200 / 1000: loss 0.453318\n",
      "iteration 300 / 1000: loss 0.451883\n",
      "iteration 400 / 1000: loss 0.466928\n",
      "iteration 500 / 1000: loss 0.421989\n",
      "iteration 600 / 1000: loss 0.441468\n",
      "iteration 700 / 1000: loss 0.432205\n",
      "iteration 800 / 1000: loss 0.488313\n",
      "iteration 900 / 1000: loss 0.403330\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098589\n",
      "iteration 100 / 1000: loss 0.456952\n",
      "iteration 200 / 1000: loss 0.480609\n",
      "iteration 300 / 1000: loss 0.477555\n",
      "iteration 400 / 1000: loss 0.413977\n",
      "iteration 500 / 1000: loss 0.462590\n",
      "iteration 600 / 1000: loss 0.443672\n",
      "iteration 700 / 1000: loss 0.493560\n",
      "iteration 800 / 1000: loss 0.483324\n",
      "iteration 900 / 1000: loss 0.422275\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100146\n",
      "iteration 100 / 1000: loss 0.472578\n",
      "iteration 200 / 1000: loss 0.459271\n",
      "iteration 300 / 1000: loss 0.428724\n",
      "iteration 400 / 1000: loss 0.442698\n",
      "iteration 500 / 1000: loss 0.417811\n",
      "iteration 600 / 1000: loss 0.484830\n",
      "iteration 700 / 1000: loss 0.457477\n",
      "iteration 800 / 1000: loss 0.426824\n",
      "iteration 900 / 1000: loss 0.444690\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097733\n",
      "iteration 100 / 1000: loss 0.461465\n",
      "iteration 200 / 1000: loss 0.351200\n",
      "iteration 300 / 1000: loss 0.396043\n",
      "iteration 400 / 1000: loss 0.446722\n",
      "iteration 500 / 1000: loss 0.442281\n",
      "iteration 600 / 1000: loss 0.423703\n",
      "iteration 700 / 1000: loss 0.434351\n",
      "iteration 800 / 1000: loss 0.474879\n",
      "iteration 900 / 1000: loss 0.425800\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097823\n",
      "iteration 100 / 1000: loss 0.473262\n",
      "iteration 200 / 1000: loss 0.492676\n",
      "iteration 300 / 1000: loss 0.448167\n",
      "iteration 400 / 1000: loss 0.450497\n",
      "iteration 500 / 1000: loss 0.501908\n",
      "iteration 600 / 1000: loss 0.402769\n",
      "iteration 700 / 1000: loss 0.459432\n",
      "iteration 800 / 1000: loss 0.489331\n",
      "iteration 900 / 1000: loss 0.452207\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099798\n",
      "iteration 100 / 1000: loss 0.472189\n",
      "iteration 200 / 1000: loss 0.483869\n",
      "iteration 300 / 1000: loss 0.448654\n",
      "iteration 400 / 1000: loss 0.429776\n",
      "iteration 500 / 1000: loss 0.479630\n",
      "iteration 600 / 1000: loss 0.449741\n",
      "iteration 700 / 1000: loss 0.485706\n",
      "iteration 800 / 1000: loss 0.494473\n",
      "iteration 900 / 1000: loss 0.431546\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095841\n",
      "iteration 100 / 1000: loss 0.428475\n",
      "iteration 200 / 1000: loss 0.471574\n",
      "iteration 300 / 1000: loss 0.538450\n",
      "iteration 400 / 1000: loss 0.457324\n",
      "iteration 500 / 1000: loss 0.475015\n",
      "iteration 600 / 1000: loss 0.489742\n",
      "iteration 700 / 1000: loss 0.455227\n",
      "iteration 800 / 1000: loss 0.482312\n",
      "iteration 900 / 1000: loss 0.513917\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098464\n",
      "iteration 100 / 1000: loss 0.527666\n",
      "iteration 200 / 1000: loss 0.473666\n",
      "iteration 300 / 1000: loss 0.507295\n",
      "iteration 400 / 1000: loss 0.505132\n",
      "iteration 500 / 1000: loss 0.475794\n",
      "iteration 600 / 1000: loss 0.512746\n",
      "iteration 700 / 1000: loss 0.497442\n",
      "iteration 800 / 1000: loss 0.485362\n",
      "iteration 900 / 1000: loss 0.467942\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098250\n",
      "iteration 100 / 1000: loss 0.456250\n",
      "iteration 200 / 1000: loss 0.482843\n",
      "iteration 300 / 1000: loss 0.468338\n",
      "iteration 400 / 1000: loss 0.529434\n",
      "iteration 500 / 1000: loss 0.470734\n",
      "iteration 600 / 1000: loss 0.503546\n",
      "iteration 700 / 1000: loss 0.504808\n",
      "iteration 800 / 1000: loss 0.440915\n",
      "iteration 900 / 1000: loss 0.484557\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099108\n",
      "iteration 100 / 1000: loss 0.429505\n",
      "iteration 200 / 1000: loss 0.455996\n",
      "iteration 300 / 1000: loss 0.459950\n",
      "iteration 400 / 1000: loss 0.524600\n",
      "iteration 500 / 1000: loss 0.455728\n",
      "iteration 600 / 1000: loss 0.462384\n",
      "iteration 700 / 1000: loss 0.476440\n",
      "iteration 800 / 1000: loss 0.472047\n",
      "iteration 900 / 1000: loss 0.464460\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100366\n",
      "iteration 100 / 1000: loss 0.451187\n",
      "iteration 200 / 1000: loss 0.468809\n",
      "iteration 300 / 1000: loss 0.492081\n",
      "iteration 400 / 1000: loss 0.413421\n",
      "iteration 500 / 1000: loss 0.448555\n",
      "iteration 600 / 1000: loss 0.420567\n",
      "iteration 700 / 1000: loss 0.447617\n",
      "iteration 800 / 1000: loss 0.413862\n",
      "iteration 900 / 1000: loss 0.430321\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099480\n",
      "iteration 100 / 1000: loss 0.429854\n",
      "iteration 200 / 1000: loss 0.390996\n",
      "iteration 300 / 1000: loss 0.440680\n",
      "iteration 400 / 1000: loss 0.438993\n",
      "iteration 500 / 1000: loss 0.402391\n",
      "iteration 600 / 1000: loss 0.433314\n",
      "iteration 700 / 1000: loss 0.401833\n",
      "iteration 800 / 1000: loss 0.467403\n",
      "iteration 900 / 1000: loss 0.435082\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100826\n",
      "iteration 100 / 1000: loss 0.436409\n",
      "iteration 200 / 1000: loss 0.422899\n",
      "iteration 300 / 1000: loss 0.472312\n",
      "iteration 400 / 1000: loss 0.407301\n",
      "iteration 500 / 1000: loss 0.432718\n",
      "iteration 600 / 1000: loss 0.450835\n",
      "iteration 700 / 1000: loss 0.464310\n",
      "iteration 800 / 1000: loss 0.436137\n",
      "iteration 900 / 1000: loss 0.467236\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099417\n",
      "iteration 100 / 1000: loss 0.504329\n",
      "iteration 200 / 1000: loss 0.375110\n",
      "iteration 300 / 1000: loss 0.500973\n",
      "iteration 400 / 1000: loss 0.463424\n",
      "iteration 500 / 1000: loss 0.447981\n",
      "iteration 600 / 1000: loss 0.411721\n",
      "iteration 700 / 1000: loss 0.474372\n",
      "iteration 800 / 1000: loss 0.398366\n",
      "iteration 900 / 1000: loss 0.431270\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098363\n",
      "iteration 100 / 1000: loss 0.414259\n",
      "iteration 200 / 1000: loss 0.475595\n",
      "iteration 300 / 1000: loss 0.456469\n",
      "iteration 400 / 1000: loss 0.474317\n",
      "iteration 500 / 1000: loss 0.448710\n",
      "iteration 600 / 1000: loss 0.480256\n",
      "iteration 700 / 1000: loss 0.474101\n",
      "iteration 800 / 1000: loss 0.469114\n",
      "iteration 900 / 1000: loss 0.536367\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099522\n",
      "iteration 100 / 1000: loss 0.469485\n",
      "iteration 200 / 1000: loss 0.423328\n",
      "iteration 300 / 1000: loss 0.459744\n",
      "iteration 400 / 1000: loss 0.438621\n",
      "iteration 500 / 1000: loss 0.490846\n",
      "iteration 600 / 1000: loss 0.458904\n",
      "iteration 700 / 1000: loss 0.487261\n",
      "iteration 800 / 1000: loss 0.452613\n",
      "iteration 900 / 1000: loss 0.516602\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099195\n",
      "iteration 100 / 1000: loss 0.500911\n",
      "iteration 200 / 1000: loss 0.486733\n",
      "iteration 300 / 1000: loss 0.471240\n",
      "iteration 400 / 1000: loss 0.464656\n",
      "iteration 500 / 1000: loss 0.504985\n",
      "iteration 600 / 1000: loss 0.461049\n",
      "iteration 700 / 1000: loss 0.465156\n",
      "iteration 800 / 1000: loss 0.382922\n",
      "iteration 900 / 1000: loss 0.423698\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098830\n",
      "iteration 100 / 1000: loss 0.463435\n",
      "iteration 200 / 1000: loss 0.481011\n",
      "iteration 300 / 1000: loss 0.467537\n",
      "iteration 400 / 1000: loss 0.496327\n",
      "iteration 500 / 1000: loss 0.494944\n",
      "iteration 600 / 1000: loss 0.475285\n",
      "iteration 700 / 1000: loss 0.432006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.500376\n",
      "iteration 900 / 1000: loss 0.420901\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098863\n",
      "iteration 100 / 1000: loss 0.487890\n",
      "iteration 200 / 1000: loss 0.467691\n",
      "iteration 300 / 1000: loss 0.458974\n",
      "iteration 400 / 1000: loss 0.461160\n",
      "iteration 500 / 1000: loss 0.474995\n",
      "iteration 600 / 1000: loss 0.469568\n",
      "iteration 700 / 1000: loss 0.492354\n",
      "iteration 800 / 1000: loss 0.471766\n",
      "iteration 900 / 1000: loss 0.456891\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099850\n",
      "iteration 100 / 1000: loss 0.489768\n",
      "iteration 200 / 1000: loss 0.473964\n",
      "iteration 300 / 1000: loss 0.477696\n",
      "iteration 400 / 1000: loss 0.480841\n",
      "iteration 500 / 1000: loss 0.451418\n",
      "iteration 600 / 1000: loss 0.454053\n",
      "iteration 700 / 1000: loss 0.474098\n",
      "iteration 800 / 1000: loss 0.516008\n",
      "iteration 900 / 1000: loss 0.430872\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100741\n",
      "iteration 100 / 1000: loss 0.496537\n",
      "iteration 200 / 1000: loss 0.445206\n",
      "iteration 300 / 1000: loss 0.431609\n",
      "iteration 400 / 1000: loss 0.471411\n",
      "iteration 500 / 1000: loss 0.481898\n",
      "iteration 600 / 1000: loss 0.479428\n",
      "iteration 700 / 1000: loss 0.448354\n",
      "iteration 800 / 1000: loss 0.458420\n",
      "iteration 900 / 1000: loss 0.485810\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098695\n",
      "iteration 100 / 1000: loss 0.477363\n",
      "iteration 200 / 1000: loss 0.491342\n",
      "iteration 300 / 1000: loss 0.443672\n",
      "iteration 400 / 1000: loss 0.528373\n",
      "iteration 500 / 1000: loss 0.469591\n",
      "iteration 600 / 1000: loss 0.494909\n",
      "iteration 700 / 1000: loss 0.487134\n",
      "iteration 800 / 1000: loss 0.450744\n",
      "iteration 900 / 1000: loss 0.467127\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097827\n",
      "iteration 100 / 1000: loss 0.468718\n",
      "iteration 200 / 1000: loss 0.457859\n",
      "iteration 300 / 1000: loss 0.493104\n",
      "iteration 400 / 1000: loss 0.413357\n",
      "iteration 500 / 1000: loss 0.462533\n",
      "iteration 600 / 1000: loss 0.474706\n",
      "iteration 700 / 1000: loss 0.515881\n",
      "iteration 800 / 1000: loss 0.484324\n",
      "iteration 900 / 1000: loss 0.495587\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098316\n",
      "iteration 100 / 1000: loss 0.445287\n",
      "iteration 200 / 1000: loss 0.434162\n",
      "iteration 300 / 1000: loss 0.419906\n",
      "iteration 400 / 1000: loss 0.467171\n",
      "iteration 500 / 1000: loss 0.418896\n",
      "iteration 600 / 1000: loss 0.410483\n",
      "iteration 700 / 1000: loss 0.420627\n",
      "iteration 800 / 1000: loss 0.442055\n",
      "iteration 900 / 1000: loss 0.423352\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099513\n",
      "iteration 100 / 1000: loss 0.467786\n",
      "iteration 200 / 1000: loss 0.464327\n",
      "iteration 300 / 1000: loss 0.502441\n",
      "iteration 400 / 1000: loss 0.451654\n",
      "iteration 500 / 1000: loss 0.452936\n",
      "iteration 600 / 1000: loss 0.462073\n",
      "iteration 700 / 1000: loss 0.424868\n",
      "iteration 800 / 1000: loss 0.466976\n",
      "iteration 900 / 1000: loss 0.397858\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097540\n",
      "iteration 100 / 1000: loss 0.441697\n",
      "iteration 200 / 1000: loss 0.402457\n",
      "iteration 300 / 1000: loss 0.478254\n",
      "iteration 400 / 1000: loss 0.407780\n",
      "iteration 500 / 1000: loss 0.451357\n",
      "iteration 600 / 1000: loss 0.464342\n",
      "iteration 700 / 1000: loss 0.415782\n",
      "iteration 800 / 1000: loss 0.458983\n",
      "iteration 900 / 1000: loss 0.425658\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099315\n",
      "iteration 100 / 1000: loss 0.431181\n",
      "iteration 200 / 1000: loss 0.411862\n",
      "iteration 300 / 1000: loss 0.463621\n",
      "iteration 400 / 1000: loss 0.479010\n",
      "iteration 500 / 1000: loss 0.437569\n",
      "iteration 600 / 1000: loss 0.416251\n",
      "iteration 700 / 1000: loss 0.442524\n",
      "iteration 800 / 1000: loss 0.435020\n",
      "iteration 900 / 1000: loss 0.433984\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099408\n",
      "iteration 100 / 1000: loss 0.469885\n",
      "iteration 200 / 1000: loss 0.481701\n",
      "iteration 300 / 1000: loss 0.436355\n",
      "iteration 400 / 1000: loss 0.409786\n",
      "iteration 500 / 1000: loss 0.475710\n",
      "iteration 600 / 1000: loss 0.402441\n",
      "iteration 700 / 1000: loss 0.482673\n",
      "iteration 800 / 1000: loss 0.426590\n",
      "iteration 900 / 1000: loss 0.480151\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100588\n",
      "iteration 100 / 1000: loss 0.443951\n",
      "iteration 200 / 1000: loss 0.465601\n",
      "iteration 300 / 1000: loss 0.484048\n",
      "iteration 400 / 1000: loss 0.427248\n",
      "iteration 500 / 1000: loss 0.492350\n",
      "iteration 600 / 1000: loss 0.462714\n",
      "iteration 700 / 1000: loss 0.484040\n",
      "iteration 800 / 1000: loss 0.429129\n",
      "iteration 900 / 1000: loss 0.428784\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100700\n",
      "iteration 100 / 1000: loss 0.458676\n",
      "iteration 200 / 1000: loss 0.554269\n",
      "iteration 300 / 1000: loss 0.453211\n",
      "iteration 400 / 1000: loss 0.454981\n",
      "iteration 500 / 1000: loss 0.517935\n",
      "iteration 600 / 1000: loss 0.427320\n",
      "iteration 700 / 1000: loss 0.476239\n",
      "iteration 800 / 1000: loss 0.481947\n",
      "iteration 900 / 1000: loss 0.442884\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099192\n",
      "iteration 100 / 1000: loss 0.455966\n",
      "iteration 200 / 1000: loss 0.463336\n",
      "iteration 300 / 1000: loss 0.478489\n",
      "iteration 400 / 1000: loss 0.480966\n",
      "iteration 500 / 1000: loss 0.435081\n",
      "iteration 600 / 1000: loss 0.480984\n",
      "iteration 700 / 1000: loss 0.454332\n",
      "iteration 800 / 1000: loss 0.482361\n",
      "iteration 900 / 1000: loss 0.475062\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095140\n",
      "iteration 100 / 1000: loss 0.443258\n",
      "iteration 200 / 1000: loss 0.422389\n",
      "iteration 300 / 1000: loss 0.456635\n",
      "iteration 400 / 1000: loss 0.488797\n",
      "iteration 500 / 1000: loss 0.415908\n",
      "iteration 600 / 1000: loss 0.476206\n",
      "iteration 700 / 1000: loss 0.504568\n",
      "iteration 800 / 1000: loss 0.440516\n",
      "iteration 900 / 1000: loss 0.464011\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098149\n",
      "iteration 100 / 1000: loss 0.448656\n",
      "iteration 200 / 1000: loss 0.418621\n",
      "iteration 300 / 1000: loss 0.453929\n",
      "iteration 400 / 1000: loss 0.514182\n",
      "iteration 500 / 1000: loss 0.477962\n",
      "iteration 600 / 1000: loss 0.469668\n",
      "iteration 700 / 1000: loss 0.503533\n",
      "iteration 800 / 1000: loss 0.431632\n",
      "iteration 900 / 1000: loss 0.442363\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100340\n",
      "iteration 100 / 1000: loss 0.507265\n",
      "iteration 200 / 1000: loss 0.481466\n",
      "iteration 300 / 1000: loss 0.471329\n",
      "iteration 400 / 1000: loss 0.511264\n",
      "iteration 500 / 1000: loss 0.492219\n",
      "iteration 600 / 1000: loss 0.482457\n",
      "iteration 700 / 1000: loss 0.501278\n",
      "iteration 800 / 1000: loss 0.490334\n",
      "iteration 900 / 1000: loss 0.457222\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097975\n",
      "iteration 100 / 1000: loss 0.476483\n",
      "iteration 200 / 1000: loss 0.447873\n",
      "iteration 300 / 1000: loss 0.523524\n",
      "iteration 400 / 1000: loss 0.480610\n",
      "iteration 500 / 1000: loss 0.452736\n",
      "iteration 600 / 1000: loss 0.496734\n",
      "iteration 700 / 1000: loss 0.457096\n",
      "iteration 800 / 1000: loss 0.472878\n",
      "iteration 900 / 1000: loss 0.499522\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098656\n",
      "iteration 100 / 1000: loss 0.496351\n",
      "iteration 200 / 1000: loss 0.487534\n",
      "iteration 300 / 1000: loss 0.493274\n",
      "iteration 400 / 1000: loss 0.469566\n",
      "iteration 500 / 1000: loss 0.474614\n",
      "iteration 600 / 1000: loss 0.434569\n",
      "iteration 700 / 1000: loss 0.508899\n",
      "iteration 800 / 1000: loss 0.479862\n",
      "iteration 900 / 1000: loss 0.487697\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100367\n",
      "iteration 100 / 1000: loss 0.403889\n",
      "iteration 200 / 1000: loss 0.429812\n",
      "iteration 300 / 1000: loss 0.397082\n",
      "iteration 400 / 1000: loss 0.389979\n",
      "iteration 500 / 1000: loss 0.424549\n",
      "iteration 600 / 1000: loss 0.385219\n",
      "iteration 700 / 1000: loss 0.458702\n",
      "iteration 800 / 1000: loss 0.444322\n",
      "iteration 900 / 1000: loss 0.423086\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098399\n",
      "iteration 100 / 1000: loss 0.487620\n",
      "iteration 200 / 1000: loss 0.424920\n",
      "iteration 300 / 1000: loss 0.442338\n",
      "iteration 400 / 1000: loss 0.448851\n",
      "iteration 500 / 1000: loss 0.376668\n",
      "iteration 600 / 1000: loss 0.449457\n",
      "iteration 700 / 1000: loss 0.424113\n",
      "iteration 800 / 1000: loss 0.391930\n",
      "iteration 900 / 1000: loss 0.495973\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098231\n",
      "iteration 100 / 1000: loss 0.419637\n",
      "iteration 200 / 1000: loss 0.451690\n",
      "iteration 300 / 1000: loss 0.457177\n",
      "iteration 400 / 1000: loss 0.436043\n",
      "iteration 500 / 1000: loss 0.453810\n",
      "iteration 600 / 1000: loss 0.425590\n",
      "iteration 700 / 1000: loss 0.461213\n",
      "iteration 800 / 1000: loss 0.469432\n",
      "iteration 900 / 1000: loss 0.460314\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099766\n",
      "iteration 100 / 1000: loss 0.449389\n",
      "iteration 200 / 1000: loss 0.440834\n",
      "iteration 300 / 1000: loss 0.416553\n",
      "iteration 400 / 1000: loss 0.419300\n",
      "iteration 500 / 1000: loss 0.471275\n",
      "iteration 600 / 1000: loss 0.352119\n",
      "iteration 700 / 1000: loss 0.451461\n",
      "iteration 800 / 1000: loss 0.444738\n",
      "iteration 900 / 1000: loss 0.424999\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098761\n",
      "iteration 100 / 1000: loss 0.430397\n",
      "iteration 200 / 1000: loss 0.464679\n",
      "iteration 300 / 1000: loss 0.438244\n",
      "iteration 400 / 1000: loss 0.508730\n",
      "iteration 500 / 1000: loss 0.441988\n",
      "iteration 600 / 1000: loss 0.409888\n",
      "iteration 700 / 1000: loss 0.452526\n",
      "iteration 800 / 1000: loss 0.410082\n",
      "iteration 900 / 1000: loss 0.391650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097365\n",
      "iteration 100 / 1000: loss 0.428001\n",
      "iteration 200 / 1000: loss 0.470387\n",
      "iteration 300 / 1000: loss 0.438675\n",
      "iteration 400 / 1000: loss 0.409877\n",
      "iteration 500 / 1000: loss 0.471684\n",
      "iteration 600 / 1000: loss 0.433433\n",
      "iteration 700 / 1000: loss 0.420482\n",
      "iteration 800 / 1000: loss 0.469182\n",
      "iteration 900 / 1000: loss 0.448864\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098345\n",
      "iteration 100 / 1000: loss 0.457853\n",
      "iteration 200 / 1000: loss 0.480032\n",
      "iteration 300 / 1000: loss 0.488312\n",
      "iteration 400 / 1000: loss 0.486580\n",
      "iteration 500 / 1000: loss 0.487486\n",
      "iteration 600 / 1000: loss 0.503501\n",
      "iteration 700 / 1000: loss 0.446958\n",
      "iteration 800 / 1000: loss 0.417057\n",
      "iteration 900 / 1000: loss 0.432868\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097218\n",
      "iteration 100 / 1000: loss 0.487191\n",
      "iteration 200 / 1000: loss 0.441859\n",
      "iteration 300 / 1000: loss 0.453498\n",
      "iteration 400 / 1000: loss 0.452856\n",
      "iteration 500 / 1000: loss 0.455103\n",
      "iteration 600 / 1000: loss 0.522197\n",
      "iteration 700 / 1000: loss 0.426714\n",
      "iteration 800 / 1000: loss 0.446733\n",
      "iteration 900 / 1000: loss 0.497005\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098297\n",
      "iteration 100 / 1000: loss 0.429886\n",
      "iteration 200 / 1000: loss 0.539208\n",
      "iteration 300 / 1000: loss 0.433746\n",
      "iteration 400 / 1000: loss 0.467351\n",
      "iteration 500 / 1000: loss 0.424757\n",
      "iteration 600 / 1000: loss 0.439989\n",
      "iteration 700 / 1000: loss 0.437696\n",
      "iteration 800 / 1000: loss 0.427669\n",
      "iteration 900 / 1000: loss 0.460417\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098106\n",
      "iteration 100 / 1000: loss 0.450934\n",
      "iteration 200 / 1000: loss 0.511748\n",
      "iteration 300 / 1000: loss 0.453668\n",
      "iteration 400 / 1000: loss 0.519088\n",
      "iteration 500 / 1000: loss 0.427540\n",
      "iteration 600 / 1000: loss 0.489286\n",
      "iteration 700 / 1000: loss 0.499078\n",
      "iteration 800 / 1000: loss 0.432958\n",
      "iteration 900 / 1000: loss 0.481579\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099026\n",
      "iteration 100 / 1000: loss 0.443525\n",
      "iteration 200 / 1000: loss 0.479200\n",
      "iteration 300 / 1000: loss 0.428023\n",
      "iteration 400 / 1000: loss 0.454661\n",
      "iteration 500 / 1000: loss 0.469364\n",
      "iteration 600 / 1000: loss 0.497438\n",
      "iteration 700 / 1000: loss 0.455604\n",
      "iteration 800 / 1000: loss 0.451897\n",
      "iteration 900 / 1000: loss 0.471816\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098441\n",
      "iteration 100 / 1000: loss 0.476999\n",
      "iteration 200 / 1000: loss 0.493161\n",
      "iteration 300 / 1000: loss 0.493916\n",
      "iteration 400 / 1000: loss 0.487680\n",
      "iteration 500 / 1000: loss 0.460516\n",
      "iteration 600 / 1000: loss 0.472989\n",
      "iteration 700 / 1000: loss 0.487618\n",
      "iteration 800 / 1000: loss 0.502584\n",
      "iteration 900 / 1000: loss 0.505560\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098232\n",
      "iteration 100 / 1000: loss 0.447499\n",
      "iteration 200 / 1000: loss 0.532725\n",
      "iteration 300 / 1000: loss 0.484225\n",
      "iteration 400 / 1000: loss 0.436401\n",
      "iteration 500 / 1000: loss 0.539154\n",
      "iteration 600 / 1000: loss 0.475174\n",
      "iteration 700 / 1000: loss 0.419537\n",
      "iteration 800 / 1000: loss 0.503620\n",
      "iteration 900 / 1000: loss 0.517335\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097724\n",
      "iteration 100 / 1000: loss 0.325126\n",
      "iteration 200 / 1000: loss 0.505404\n",
      "iteration 300 / 1000: loss 0.432602\n",
      "iteration 400 / 1000: loss 0.435924\n",
      "iteration 500 / 1000: loss 0.402499\n",
      "iteration 600 / 1000: loss 0.405961\n",
      "iteration 700 / 1000: loss 0.464717\n",
      "iteration 800 / 1000: loss 0.488104\n",
      "iteration 900 / 1000: loss 0.384834\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096131\n",
      "iteration 100 / 1000: loss 0.462552\n",
      "iteration 200 / 1000: loss 0.460365\n",
      "iteration 300 / 1000: loss 0.467189\n",
      "iteration 400 / 1000: loss 0.393217\n",
      "iteration 500 / 1000: loss 0.452513\n",
      "iteration 600 / 1000: loss 0.418763\n",
      "iteration 700 / 1000: loss 0.452453\n",
      "iteration 800 / 1000: loss 0.493750\n",
      "iteration 900 / 1000: loss 0.426518\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098626\n",
      "iteration 100 / 1000: loss 0.402818\n",
      "iteration 200 / 1000: loss 0.468100\n",
      "iteration 300 / 1000: loss 0.415429\n",
      "iteration 400 / 1000: loss 0.382882\n",
      "iteration 500 / 1000: loss 0.461566\n",
      "iteration 600 / 1000: loss 0.400143\n",
      "iteration 700 / 1000: loss 0.462259\n",
      "iteration 800 / 1000: loss 0.430618\n",
      "iteration 900 / 1000: loss 0.444321\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.101438\n",
      "iteration 100 / 1000: loss 0.454997\n",
      "iteration 200 / 1000: loss 0.449420\n",
      "iteration 300 / 1000: loss 0.404797\n",
      "iteration 400 / 1000: loss 0.472789\n",
      "iteration 500 / 1000: loss 0.461179\n",
      "iteration 600 / 1000: loss 0.419479\n",
      "iteration 700 / 1000: loss 0.429833\n",
      "iteration 800 / 1000: loss 0.451811\n",
      "iteration 900 / 1000: loss 0.431545\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100924\n",
      "iteration 100 / 1000: loss 0.460623\n",
      "iteration 200 / 1000: loss 0.409799\n",
      "iteration 300 / 1000: loss 0.484613\n",
      "iteration 400 / 1000: loss 0.463596\n",
      "iteration 500 / 1000: loss 0.466859\n",
      "iteration 600 / 1000: loss 0.440155\n",
      "iteration 700 / 1000: loss 0.453939\n",
      "iteration 800 / 1000: loss 0.450839\n",
      "iteration 900 / 1000: loss 0.420424\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099268\n",
      "iteration 100 / 1000: loss 0.475211\n",
      "iteration 200 / 1000: loss 0.427387\n",
      "iteration 300 / 1000: loss 0.444375\n",
      "iteration 400 / 1000: loss 0.495643\n",
      "iteration 500 / 1000: loss 0.471967\n",
      "iteration 600 / 1000: loss 0.450115\n",
      "iteration 700 / 1000: loss 0.422956\n",
      "iteration 800 / 1000: loss 0.416476\n",
      "iteration 900 / 1000: loss 0.460894\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098861\n",
      "iteration 100 / 1000: loss 0.487174\n",
      "iteration 200 / 1000: loss 0.419122\n",
      "iteration 300 / 1000: loss 0.508062\n",
      "iteration 400 / 1000: loss 0.490286\n",
      "iteration 500 / 1000: loss 0.518245\n",
      "iteration 600 / 1000: loss 0.470598\n",
      "iteration 700 / 1000: loss 0.453993\n",
      "iteration 800 / 1000: loss 0.448943\n",
      "iteration 900 / 1000: loss 0.420141\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098288\n",
      "iteration 100 / 1000: loss 0.467117\n",
      "iteration 200 / 1000: loss 0.498420\n",
      "iteration 300 / 1000: loss 0.476952\n",
      "iteration 400 / 1000: loss 0.445924\n",
      "iteration 500 / 1000: loss 0.434615\n",
      "iteration 600 / 1000: loss 0.472696\n",
      "iteration 700 / 1000: loss 0.420336\n",
      "iteration 800 / 1000: loss 0.462077\n",
      "iteration 900 / 1000: loss 0.422113\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098848\n",
      "iteration 100 / 1000: loss 0.469319\n",
      "iteration 200 / 1000: loss 0.448276\n",
      "iteration 300 / 1000: loss 0.452010\n",
      "iteration 400 / 1000: loss 0.461228\n",
      "iteration 500 / 1000: loss 0.448040\n",
      "iteration 600 / 1000: loss 0.450182\n",
      "iteration 700 / 1000: loss 0.498434\n",
      "iteration 800 / 1000: loss 0.447574\n",
      "iteration 900 / 1000: loss 0.471220\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099759\n",
      "iteration 100 / 1000: loss 0.502054\n",
      "iteration 200 / 1000: loss 0.457422\n",
      "iteration 300 / 1000: loss 0.469513\n",
      "iteration 400 / 1000: loss 0.511977\n",
      "iteration 500 / 1000: loss 0.461085\n",
      "iteration 600 / 1000: loss 0.434627\n",
      "iteration 700 / 1000: loss 0.492131\n",
      "iteration 800 / 1000: loss 0.474514\n",
      "iteration 900 / 1000: loss 0.439514\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.094937\n",
      "iteration 100 / 1000: loss 0.474385\n",
      "iteration 200 / 1000: loss 0.469931\n",
      "iteration 300 / 1000: loss 0.434812\n",
      "iteration 400 / 1000: loss 0.450440\n",
      "iteration 500 / 1000: loss 0.470166\n",
      "iteration 600 / 1000: loss 0.506122\n",
      "iteration 700 / 1000: loss 0.482827\n",
      "iteration 800 / 1000: loss 0.517930\n",
      "iteration 900 / 1000: loss 0.452361\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100532\n",
      "iteration 100 / 1000: loss 0.491641\n",
      "iteration 200 / 1000: loss 0.502201\n",
      "iteration 300 / 1000: loss 0.479385\n",
      "iteration 400 / 1000: loss 0.439616\n",
      "iteration 500 / 1000: loss 0.494499\n",
      "iteration 600 / 1000: loss 0.493339\n",
      "iteration 700 / 1000: loss 0.443864\n",
      "iteration 800 / 1000: loss 0.476523\n",
      "iteration 900 / 1000: loss 0.444102\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.100413\n",
      "iteration 100 / 1000: loss 0.454713\n",
      "iteration 200 / 1000: loss 0.488610\n",
      "iteration 300 / 1000: loss 0.482409\n",
      "iteration 400 / 1000: loss 0.490282\n",
      "iteration 500 / 1000: loss 0.498458\n",
      "iteration 600 / 1000: loss 0.447445\n",
      "iteration 700 / 1000: loss 0.455232\n",
      "iteration 800 / 1000: loss 0.440829\n",
      "iteration 900 / 1000: loss 0.461649\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.097292\n",
      "iteration 100 / 1000: loss 0.440215\n",
      "iteration 200 / 1000: loss 0.398756\n",
      "iteration 300 / 1000: loss 0.449249\n",
      "iteration 400 / 1000: loss 0.419649\n",
      "iteration 500 / 1000: loss 0.433578\n",
      "iteration 600 / 1000: loss 0.410295\n",
      "iteration 700 / 1000: loss 0.452170\n",
      "iteration 800 / 1000: loss 0.411229\n",
      "iteration 900 / 1000: loss 0.466377\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099407\n",
      "iteration 100 / 1000: loss 0.412334\n",
      "iteration 200 / 1000: loss 0.442163\n",
      "iteration 300 / 1000: loss 0.490625\n",
      "iteration 400 / 1000: loss 0.425779\n",
      "iteration 500 / 1000: loss 0.405618\n",
      "iteration 600 / 1000: loss 0.457140\n",
      "iteration 700 / 1000: loss 0.376174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 0.449186\n",
      "iteration 900 / 1000: loss 0.416125\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098742\n",
      "iteration 100 / 1000: loss 0.431053\n",
      "iteration 200 / 1000: loss 0.436781\n",
      "iteration 300 / 1000: loss 0.462017\n",
      "iteration 400 / 1000: loss 0.481485\n",
      "iteration 500 / 1000: loss 0.447785\n",
      "iteration 600 / 1000: loss 0.452421\n",
      "iteration 700 / 1000: loss 0.473601\n",
      "iteration 800 / 1000: loss 0.476131\n",
      "iteration 900 / 1000: loss 0.407183\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096440\n",
      "iteration 100 / 1000: loss 0.466333\n",
      "iteration 200 / 1000: loss 0.471472\n",
      "iteration 300 / 1000: loss 0.439181\n",
      "iteration 400 / 1000: loss 0.424269\n",
      "iteration 500 / 1000: loss 0.513390\n",
      "iteration 600 / 1000: loss 0.411680\n",
      "iteration 700 / 1000: loss 0.426776\n",
      "iteration 800 / 1000: loss 0.413459\n",
      "iteration 900 / 1000: loss 0.486587\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098603\n",
      "iteration 100 / 1000: loss 0.449388\n",
      "iteration 200 / 1000: loss 0.461904\n",
      "iteration 300 / 1000: loss 0.424935\n",
      "iteration 400 / 1000: loss 0.412625\n",
      "iteration 500 / 1000: loss 0.444279\n",
      "iteration 600 / 1000: loss 0.500437\n",
      "iteration 700 / 1000: loss 0.434441\n",
      "iteration 800 / 1000: loss 0.413841\n",
      "iteration 900 / 1000: loss 0.442848\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099645\n",
      "iteration 100 / 1000: loss 0.472927\n",
      "iteration 200 / 1000: loss 0.450529\n",
      "iteration 300 / 1000: loss 0.443655\n",
      "iteration 400 / 1000: loss 0.477770\n",
      "iteration 500 / 1000: loss 0.458469\n",
      "iteration 600 / 1000: loss 0.448014\n",
      "iteration 700 / 1000: loss 0.444222\n",
      "iteration 800 / 1000: loss 0.450011\n",
      "iteration 900 / 1000: loss 0.431806\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099674\n",
      "iteration 100 / 1000: loss 0.449924\n",
      "iteration 200 / 1000: loss 0.411580\n",
      "iteration 300 / 1000: loss 0.449980\n",
      "iteration 400 / 1000: loss 0.441834\n",
      "iteration 500 / 1000: loss 0.460530\n",
      "iteration 600 / 1000: loss 0.472917\n",
      "iteration 700 / 1000: loss 0.485852\n",
      "iteration 800 / 1000: loss 0.491635\n",
      "iteration 900 / 1000: loss 0.473823\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.095549\n",
      "iteration 100 / 1000: loss 0.433947\n",
      "iteration 200 / 1000: loss 0.465471\n",
      "iteration 300 / 1000: loss 0.451539\n",
      "iteration 400 / 1000: loss 0.422150\n",
      "iteration 500 / 1000: loss 0.414114\n",
      "iteration 600 / 1000: loss 0.446550\n",
      "iteration 700 / 1000: loss 0.469562\n",
      "iteration 800 / 1000: loss 0.463717\n",
      "iteration 900 / 1000: loss 0.455726\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.096548\n",
      "iteration 100 / 1000: loss 0.430493\n",
      "iteration 200 / 1000: loss 0.485303\n",
      "iteration 300 / 1000: loss 0.472033\n",
      "iteration 400 / 1000: loss 0.452764\n",
      "iteration 500 / 1000: loss 0.474522\n",
      "iteration 600 / 1000: loss 0.479331\n",
      "iteration 700 / 1000: loss 0.465429\n",
      "iteration 800 / 1000: loss 0.482514\n",
      "iteration 900 / 1000: loss 0.431104\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.098056\n",
      "iteration 100 / 1000: loss 0.463748\n",
      "iteration 200 / 1000: loss 0.454034\n",
      "iteration 300 / 1000: loss 0.445110\n",
      "iteration 400 / 1000: loss 0.462214\n",
      "iteration 500 / 1000: loss 0.491320\n",
      "iteration 600 / 1000: loss 0.434163\n",
      "iteration 700 / 1000: loss 0.433556\n",
      "iteration 800 / 1000: loss 0.452381\n",
      "iteration 900 / 1000: loss 0.525797\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099535\n",
      "iteration 100 / 1000: loss 0.459651\n",
      "iteration 200 / 1000: loss 0.469922\n",
      "iteration 300 / 1000: loss 0.484616\n",
      "iteration 400 / 1000: loss 0.446911\n",
      "iteration 500 / 1000: loss 0.486904\n",
      "iteration 600 / 1000: loss 0.439955\n",
      "iteration 700 / 1000: loss 0.481328\n",
      "iteration 800 / 1000: loss 0.466680\n",
      "iteration 900 / 1000: loss 0.512645\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099964\n",
      "iteration 100 / 1000: loss 0.436709\n",
      "iteration 200 / 1000: loss 0.440005\n",
      "iteration 300 / 1000: loss 0.462297\n",
      "iteration 400 / 1000: loss 0.444065\n",
      "iteration 500 / 1000: loss 0.429275\n",
      "iteration 600 / 1000: loss 0.454560\n",
      "iteration 700 / 1000: loss 0.528238\n",
      "iteration 800 / 1000: loss 0.472852\n",
      "iteration 900 / 1000: loss 0.485278\n",
      "(15,)\n",
      "iteration 0 / 1000: loss 1.099477\n",
      "iteration 100 / 1000: loss 0.460490\n",
      "iteration 200 / 1000: loss 0.552196\n",
      "iteration 300 / 1000: loss 0.460841\n",
      "iteration 400 / 1000: loss 0.482538\n",
      "iteration 500 / 1000: loss 0.504676\n",
      "iteration 600 / 1000: loss 0.504536\n",
      "iteration 700 / 1000: loss 0.446529\n",
      "iteration 800 / 1000: loss 0.513926\n",
      "iteration 900 / 1000: loss 0.423278\n",
      "(15,)\n",
      "lr 6.000000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.000000e-01 reg 2.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 2.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.000000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.000000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 3.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 3.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.000000e-01 reg 4.100000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 4.300000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.000000e-01 reg 4.500000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 4.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.000000e-01 reg 4.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.100000e-01 reg 2.700000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 2.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 3.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 3.300000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.100000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 4.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.100000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.100000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.100000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 2.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 3.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.200000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 4.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.200000e-01 reg 4.700000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.200000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.300000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 2.700000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.300000e-01 reg 3.100000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.300000e-01 reg 3.300000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 3.900000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.300000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.300000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 2.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.400000e-01 reg 3.100000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 3.300000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.400000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.400000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.400000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.400000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 2.500000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 2.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.500000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.500000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.500000e-01 reg 3.300000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 6.500000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.500000e-01 reg 3.700000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.500000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.733333\n",
      "lr 6.500000e-01 reg 4.900000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.600000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 2.700000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 6.600000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 3.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.600000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 4.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 4.300000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.600000e-01 reg 4.500000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.600000e-01 reg 4.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.600000e-01 reg 4.900000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 2.700000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 6.700000e-01 reg 2.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.700000e-01 reg 3.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 3.300000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 3.500000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 3.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.700000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.700000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 2.500000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 2.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 3.300000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.800000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.800000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.800000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.800000e-01 reg 4.500000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.800000e-01 reg 4.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 6.900000e-01 reg 2.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.900000e-01 reg 2.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 3.300000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 3.700000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 6.900000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 6.900000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 6.900000e-01 reg 4.900000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.000000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.000000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 3.500000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.000000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 4.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.000000e-01 reg 4.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.000000e-01 reg 4.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 2.500000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.100000e-01 reg 2.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.100000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.100000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.100000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.100000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 4.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 4.300000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.100000e-01 reg 4.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.733333\n",
      "lr 7.100000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.200000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.200000e-01 reg 3.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.200000e-01 reg 3.300000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 3.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.200000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.200000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.200000e-01 reg 4.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.200000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.300000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.300000e-01 reg 2.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 2.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 3.300000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.300000e-01 reg 3.500000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 3.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 4.500000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.300000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.400000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.866667\n",
      "lr 7.400000e-01 reg 3.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 3.300000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.400000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.400000e-01 reg 4.300000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.400000e-01 reg 4.500000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.400000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.400000e-01 reg 4.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 2.700000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.500000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.500000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.500000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.500000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.500000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.500000e-01 reg 4.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 4.700000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.500000e-01 reg 4.900000e-02 train accuracy: 0.840000  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 2.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.600000e-01 reg 2.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.600000e-01 reg 3.100000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 7.600000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 3.700000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.600000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 4.300000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 4.500000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 4.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.600000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.700000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 2.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 3.300000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 3.900000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.700000e-01 reg 4.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.700000e-01 reg 4.300000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.700000e-01 reg 4.500000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 7.700000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.700000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 2.500000e-02 train accuracy: 0.906667  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 2.700000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.800000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 3.300000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.800000e-01 reg 3.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.800000e-01 reg 3.900000e-02 train accuracy: 0.880000  val accuracy: 0.800000\n",
      "lr 7.800000e-01 reg 4.100000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 4.300000e-02 train accuracy: 0.853333  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 4.500000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.800000e-01 reg 4.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.800000e-01 reg 4.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 2.500000e-02 train accuracy: 0.906667  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 2.700000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.900000e-01 reg 2.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 3.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 7.900000e-01 reg 3.300000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 3.500000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 7.900000e-01 reg 3.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 3.900000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 4.100000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 4.500000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 4.700000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "lr 7.900000e-01 reg 4.900000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 2.500000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 8.000000e-01 reg 2.700000e-02 train accuracy: 0.893333  val accuracy: 0.800000\n",
      "lr 8.000000e-01 reg 2.900000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 3.100000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 3.300000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 3.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 3.700000e-02 train accuracy: 0.880000  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 3.900000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 8.000000e-01 reg 4.100000e-02 train accuracy: 0.866667  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 4.300000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 8.000000e-01 reg 4.500000e-02 train accuracy: 0.893333  val accuracy: 0.733333\n",
      "lr 8.000000e-01 reg 4.700000e-02 train accuracy: 0.866667  val accuracy: 0.800000\n",
      "lr 8.000000e-01 reg 4.900000e-02 train accuracy: 0.853333  val accuracy: 0.800000\n",
      "best validation accuracy achieved during train: 0.906667\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [6e-1, 8e-1]\n",
    "regularization_strengths = [2.5e-2,5e-2]\n",
    "\n",
    "for lr in np.arange(learning_rates[0],learning_rates[1],1e-2):\n",
    "    for reg in np.arange(regularization_strengths[0],regularization_strengths[1],2e-3):\n",
    "        softmax = Softmax()   \n",
    "        #如果这一步不写，就会之前训练的W矩阵接着训练，自然是效果越来越好，num_iters不能太小，否则很难收敛\n",
    "        loss_hist = softmax.train(x_train, t_train, lr, reg,num_iters=1000,verbose=True)  #这部分还是要好好看\n",
    "        t_val_pred = softmax.predict(x_val)\n",
    "        print(t_val_pred.shape)\n",
    "        t_train_pred = softmax.predict(x_train)\n",
    "        results[(lr,reg)] =  [np.mean(t_train == t_train_pred) , np.mean(t_val == t_val_pred) ]     #accurancy 原来是字典形式赋值\n",
    "        for l in results.values():\n",
    "            if l[0] > best_val:\n",
    "                best_val = l[0]\n",
    "                best_softmax = softmax\n",
    "                best_w = best_softmax.W\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f  val accuracy: %f' % (\n",
    "                  lr, reg, train_accuracy,val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during train: %f' % best_val)\n",
    "#做测试集\n",
    "t_test_pred = best_softmax.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f537da5da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 75+10000个点一起画\n",
    "# plt.scatter(np.vstack((x_train,x_test))[:, 0], np.vstack((x_train,x_test))[:, 1], c =np.append(t_train,t_test_pred))  # maybe the color\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], c=t_test_pred)  # maybe the color\n",
    "plt.xlim([-4, 6])\n",
    "plt.ylim([-4, 6])\n",
    "# 之前均值化了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"t_test_1.npy\",t_test_pred)# 准确率不高，重新跑几遍可能就好了，因为那个训练集什么的是随机选的，这里训练样本实在太少，所以不具有代表性，反倒是有1万个左右的测试集\n",
    "# 训练集和验证集比例可以调整一下\n",
    "# 最开始不错的那份是t_test.npy,所以再存的都是叫t_test_1.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.77395006  0.0283039   0.74564616]\n",
      " [-0.9570053  -0.10185471  1.05886001]\n",
      " [-0.4228452   1.05157067 -0.62872547]]\n"
     ]
    }
   ],
   "source": [
    "print(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(t_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4, 6)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f537d9788d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 这一段检验要把前面的改了\n",
    "# t_train_pred = best_softmax.predict(x_train)\n",
    "# x_train = np.load('x_train.npy')\n",
    "# plt.scatter(x_train[:, 0], x_train[:, 1], c=t_train_pred)  # maybe the color\n",
    "# plt.xlim([-4, 6])\n",
    "# plt.ylim([-4, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.89985978 2.03252308]\n"
     ]
    }
   ],
   "source": [
    "print(mean_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
